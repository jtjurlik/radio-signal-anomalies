{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29640,
     "status": "ok",
     "timestamp": 1731273655196,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "dD5zQDdqMWLa",
    "outputId": "838700af-7ddd-4bd8-d571-15b198f760e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4978,
     "status": "ok",
     "timestamp": 1731273660169,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "PNY35IV9PKtY",
    "outputId": "c64d2bde-f679-41f1-e92f-783fc2719c86"
   },
   "outputs": [],
   "source": [
    "pip install keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dazA9FqwLJie"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Dense, Flatten, Dropout, Input, MaxPooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import utils\n",
    "import keras_tuner as kt\n",
    "\n",
    "utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjTCMXai53r8"
   },
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "LOOKBACK = 24\n",
    "HORIZON = 24\n",
    "N_SPLITS = 4\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPkJYqv65-8O"
   },
   "source": [
    "# Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldcRlogr5843"
   },
   "outputs": [],
   "source": [
    "# Time series split function (Expanding Window)\n",
    "def time_series_split(df, n_splits=N_SPLITS, test_size=0.2):\n",
    "    df = df.sort_values('timestamp')\n",
    "    test_split_index = int(len(df) * (1 - test_size))\n",
    "    train_val_df = df.iloc[:test_split_index]\n",
    "    test_df = df.iloc[test_split_index:]\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    splits = [(train_val_df.iloc[train_index], train_val_df.iloc[val_index]) for train_index, val_index in tscv.split(train_val_df)]\n",
    "    return splits, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oS3gb2T6Po6"
   },
   "outputs": [],
   "source": [
    "# Sequence creation for univariate time series\n",
    "def create_sequences(df, lookback=LOOKBACK, horizon=HORIZON):\n",
    "    X, y, anomaly, cell_id = [], [], [], []\n",
    "\n",
    "    # Loop through each unique cell in the dataset\n",
    "    for cell in df['cell'].unique():\n",
    "        # Filter the dataframe for the current cell only\n",
    "        cell_df = df[df['cell'] == cell]\n",
    "\n",
    "        # Generate sequences within this cell's data\n",
    "        for i in range(lookback, len(cell_df) - horizon + 1):\n",
    "            # Lookback sequence for minRSSI only (univariate)\n",
    "            X_seq = cell_df.iloc[i - lookback:i][['minRSSI']].values\n",
    "            # Target horizon sequence for minRSSI\n",
    "            y_seq = cell_df.iloc[i:i + horizon]['minRSSI'].values\n",
    "            # Anomaly sequences for later evaluation\n",
    "            anomaly_seq = cell_df.iloc[i:i + horizon]['anomaly'].values\n",
    "            # Cell ID for each sequence\n",
    "            cell_seq = cell_df.iloc[i:i + horizon]['cell'].values\n",
    "\n",
    "            # Append sequences to output lists\n",
    "            X.append(X_seq)\n",
    "            y.append(y_seq)\n",
    "            anomaly.append(anomaly_seq)\n",
    "            cell_id.append(cell_seq)\n",
    "\n",
    "    # Convert lists to numpy arrays for model input\n",
    "    return np.array(X), np.array(y), np.array(anomaly), np.array(cell_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHHpMuv8MBqO"
   },
   "outputs": [],
   "source": [
    "def tune_hyperparameters_hyperband(X_train, y_train, param_grid, max_epochs=30):\n",
    "    # Define model building function for Keras Tuner\n",
    "    def build_tunable_1dcnn(hp):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(LOOKBACK, 1)))\n",
    "\n",
    "        # Add Conv1D layers based on tunable parameters\n",
    "        for i in range(hp.Int('num_conv_layers', min_value=1, max_value=4)):\n",
    "            model.add(Conv1D(\n",
    "                filters=hp.Choice('num_filters', values=[16, 32, 64, 128]),\n",
    "                kernel_size=hp.Choice('filter_size', values=[1, 3, 5, 7]),\n",
    "                activation='relu'\n",
    "            ))\n",
    "\n",
    "            # Optional pooling layer\n",
    "            if hp.Boolean(f'add_pooling_{i}'):\n",
    "                pool_size = hp.Choice(f'pool_size_{i}', values=[2, 3])\n",
    "                model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "            # Optional dropout layer\n",
    "            if hp.Boolean(f'use_dropout_{i}'):\n",
    "                dropout_rate = hp.Float(f'dropout_rate_{i}', min_value=0.2, max_value=0.3, step=0.1)\n",
    "                model.add(Dropout(dropout_rate))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(HORIZON))\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='mse')\n",
    "        return model\n",
    "\n",
    "    # Hyperband tuner instance\n",
    "    tuner = kt.Hyperband(\n",
    "        hypermodel=build_tunable_1dcnn,\n",
    "        objective='val_loss',\n",
    "        max_epochs=max_epochs,\n",
    "        factor=3,\n",
    "        directory='/content/drive/MyDrive/Thesis/Thesis/cnn/univar_tuning',\n",
    "        project_name='1dcnn_tuning'\n",
    "    )\n",
    "\n",
    "    # Fit Hyperband tuner to training data\n",
    "    tuner.search(X_train, y_train, epochs=max_epochs, batch_size=BATCH_SIZE, validation_split=0.2, verbose=1)\n",
    "\n",
    "    # Get best model and hyperparameters\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_params = tuner.get_best_hyperparameters(num_trials=1)[0].values\n",
    "\n",
    "    return best_model, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isvcolF8_kWu"
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 4415,
     "status": "ok",
     "timestamp": 1731273670444,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "vW_sPK1f_6Xs",
    "outputId": "481c0264-972e-4753-dbb6-2144a16b4eed"
   },
   "outputs": [],
   "source": [
    "imp_folder = os.getenv(\"DATA_PATH\", \"./default_data_path/\")\n",
    "exp_folder = os.getenv(\"MODEL_PATH\", \"./default_model_path/\")\n",
    "\n",
    "df = pd.read_csv(imp_folder + 'cell_undersampled_1.csv')\n",
    "df = df[['timestamp', 'cell', 'minRSSI', 'anomaly']]\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1196,
     "status": "ok",
     "timestamp": 1731273671632,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "LunGFPbs_ZEE",
    "outputId": "b6a764fd-ffab-4dd5-e9ef-42e2fcae86d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1:\n",
      "  Train set shape: (151460, 4)\n",
      "  Validation set shape: (151459, 4)\n",
      "Split 2:\n",
      "  Train set shape: (302919, 4)\n",
      "  Validation set shape: (151459, 4)\n",
      "Split 3:\n",
      "  Train set shape: (454378, 4)\n",
      "  Validation set shape: (151459, 4)\n",
      "Split 4:\n",
      "  Train set shape: (605837, 4)\n",
      "  Validation set shape: (151459, 4)\n",
      "Test set shape: (189324, 4)\n"
     ]
    }
   ],
   "source": [
    "splits, test_set = time_series_split(df, 4)\n",
    "\n",
    "for i, (train, val) in enumerate(splits):\n",
    "    print(f\"Split {i + 1}:\")\n",
    "    print(f\"  Train set shape: {train.shape}\")\n",
    "    print(f\"  Validation set shape: {val.shape}\")\n",
    "\n",
    "print(f\"Test set shape: {test_set.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SueCamVvA7Wp"
   },
   "source": [
    "# Run the tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1676171,
     "status": "ok",
     "timestamp": 1731275372768,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "6qXfY5FjA8aH",
    "outputId": "131feca9-f3d7-4bd3-9166-3fd13be0a124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split 1/4\n",
      "Reloading Tuner from /content/drive/MyDrive/Thesis/Thesis/cnn/univar_tuning/1dcnn_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4321/4321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step\n",
      "Split 1 Results:\n",
      "  Best parameters: {'num_conv_layers': 3, 'num_filters': 64, 'filter_size': 5, 'use_dropout': False, 'dropout_rate': 0.2, 'tuner/epochs': 2, 'tuner/initial_epoch': 0, 'tuner/bracket': 3, 'tuner/round': 0, 'add_pooling_0': False, 'use_dropout_0': False, 'add_pooling_1': False, 'use_dropout_1': False, 'add_pooling_2': False, 'use_dropout_2': False}\n",
      "  Validation RMSE: 0.6324001606953417\n",
      "  Validation MAE: 0.3928407542427677\n",
      "Processing split 2/4\n",
      "Reloading Tuner from /content/drive/MyDrive/Thesis/Thesis/cnn/univar_tuning/1dcnn_tuning/tuner0.json\n",
      "\u001b[1m   1/4321\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:10\u001b[0m 127ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4321/4321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step\n",
      "Split 2 Results:\n",
      "  Best parameters: {'num_conv_layers': 3, 'num_filters': 64, 'filter_size': 5, 'use_dropout': False, 'dropout_rate': 0.2, 'tuner/epochs': 2, 'tuner/initial_epoch': 0, 'tuner/bracket': 3, 'tuner/round': 0, 'add_pooling_0': False, 'use_dropout_0': False, 'add_pooling_1': False, 'use_dropout_1': False, 'add_pooling_2': False, 'use_dropout_2': False}\n",
      "  Validation RMSE: 0.4020272192084934\n",
      "  Validation MAE: 0.23952492974052864\n",
      "Processing split 3/4\n",
      "Reloading Tuner from /content/drive/MyDrive/Thesis/Thesis/cnn/univar_tuning/1dcnn_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4321/4321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step\n",
      "Split 3 Results:\n",
      "  Best parameters: {'num_conv_layers': 3, 'num_filters': 64, 'filter_size': 5, 'use_dropout': False, 'dropout_rate': 0.2, 'tuner/epochs': 2, 'tuner/initial_epoch': 0, 'tuner/bracket': 3, 'tuner/round': 0, 'add_pooling_0': False, 'use_dropout_0': False, 'add_pooling_1': False, 'use_dropout_1': False, 'add_pooling_2': False, 'use_dropout_2': False}\n",
      "  Validation RMSE: 0.5591703237498701\n",
      "  Validation MAE: 0.3439769742082885\n",
      "Processing split 4/4\n",
      "Reloading Tuner from /content/drive/MyDrive/Thesis/Thesis/cnn/univar_tuning/1dcnn_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4321/4321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 4 Results:\n",
      "  Best parameters: {'num_conv_layers': 3, 'num_filters': 64, 'filter_size': 5, 'use_dropout': False, 'dropout_rate': 0.2, 'tuner/epochs': 2, 'tuner/initial_epoch': 0, 'tuner/bracket': 3, 'tuner/round': 0, 'add_pooling_0': False, 'use_dropout_0': False, 'add_pooling_1': False, 'use_dropout_1': False, 'add_pooling_2': False, 'use_dropout_2': False}\n",
      "  Validation RMSE: 0.9752770029700313\n",
      "  Validation MAE: 0.5863544068307599\n",
      "Saved the best model from the last split.\n",
      "\n",
      "All best hyperparameters across splits: [{'num_conv_layers': 3, 'num_filters': 64, 'filter_size': 5, 'use_dropout': False, 'dropout_rate': 0.2, 'tuner/epochs': 2, 'tuner/initial_epoch': 0, 'tuner/bracket': 3, 'tuner/round': 0, 'add_pooling_0': False, 'use_dropout_0': False, 'add_pooling_1': False, 'use_dropout_1': False, 'add_pooling_2': False, 'use_dropout_2': False}, {'num_conv_layers': 3, 'num_filters': 64, 'filter_size': 5, 'use_dropout': False, 'dropout_rate': 0.2, 'tuner/epochs': 2, 'tuner/initial_epoch': 0, 'tuner/bracket': 3, 'tuner/round': 0, 'add_pooling_0': False, 'use_dropout_0': False, 'add_pooling_1': False, 'use_dropout_1': False, 'add_pooling_2': False, 'use_dropout_2': False}, {'num_conv_layers': 3, 'num_filters': 64, 'filter_size': 5, 'use_dropout': False, 'dropout_rate': 0.2, 'tuner/epochs': 2, 'tuner/initial_epoch': 0, 'tuner/bracket': 3, 'tuner/round': 0, 'add_pooling_0': False, 'use_dropout_0': False, 'add_pooling_1': False, 'use_dropout_1': False, 'add_pooling_2': False, 'use_dropout_2': False}, {'num_conv_layers': 3, 'num_filters': 64, 'filter_size': 5, 'use_dropout': False, 'dropout_rate': 0.2, 'tuner/epochs': 2, 'tuner/initial_epoch': 0, 'tuner/bracket': 3, 'tuner/round': 0, 'add_pooling_0': False, 'use_dropout_0': False, 'add_pooling_1': False, 'use_dropout_1': False, 'add_pooling_2': False, 'use_dropout_2': False}]\n"
     ]
    }
   ],
   "source": [
    "all_best_params = []\n",
    "\n",
    "for i, (train_set, val_set) in enumerate(splits):\n",
    "    print(f\"Processing split {i + 1}/{N_SPLITS}\")\n",
    "\n",
    "    # Preprocess the data (fit scaler on train, transform on train and val)\n",
    "    train_set['minRSSI'] = scaler.fit_transform(train_set[['minRSSI']])\n",
    "    val_set['minRSSI'] = scaler.transform(val_set[['minRSSI']])\n",
    "\n",
    "    # Create sequences for train and validation sets\n",
    "    X_train, y_train, _, _ = create_sequences(train_set)\n",
    "    X_val, y_val, _, _ = create_sequences(val_set)\n",
    "\n",
    "    # Reshape X for 1D CNN (samples, timesteps, features)\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "\n",
    "    # Tune and train the model\n",
    "    best_model, best_params = tune_hyperparameters_hyperband(X_train, y_train, param_grid=None)\n",
    "    all_best_params.append(best_params)\n",
    "\n",
    "    # Evaluate the model on validation set\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "    print(f\"Split {i + 1} Results:\")\n",
    "    print(f\"  Best parameters: {best_params}\")\n",
    "    print(f\"  Validation RMSE: {val_rmse}\")\n",
    "    print(f\"  Validation MAE: {val_mae}\")\n",
    "\n",
    "    # Save the best model for the last split only\n",
    "    if i == len(splits) - 1:\n",
    "        best_model.save(os.path.join(exp_folder, 'best_1dcnn_model.h5'))\n",
    "        print(\"Saved the best model from the last split.\")\n",
    "\n",
    "print()\n",
    "print(\"All best hyperparameters across splits:\", all_best_params)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
