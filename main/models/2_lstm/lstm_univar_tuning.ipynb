{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19988,
     "status": "ok",
     "timestamp": 1731327499451,
     "user": {
      "displayName": "Jakob",
      "userId": "17712631153690750491"
     },
     "user_tz": -60
    },
    "id": "dD5zQDdqMWLa",
    "outputId": "a96a7840-c52c-4214-c9c2-402bb33d267f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3240,
     "status": "ok",
     "timestamp": 1731327502685,
     "user": {
      "displayName": "Jakob",
      "userId": "17712631153690750491"
     },
     "user_tz": -60
    },
    "id": "PNY35IV9PKtY",
    "outputId": "c58138eb-511f-4bd0-8dc9-6dbabbf15b4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tuner\n",
      "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.4.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
      "Collecting kt-legacy (from keras-tuner)\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.26.4)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.9.3)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.8)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.12.1)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.8.30)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
      "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dazA9FqwLJie"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import utils\n",
    "import keras_tuner as kt\n",
    "\n",
    "utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjTCMXai53r8"
   },
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "LOOKBACK = 24\n",
    "HORIZON = 24\n",
    "N_SPLITS = 4\n",
    "EPOCHS = 10\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPkJYqv65-8O"
   },
   "source": [
    "# Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldcRlogr5843"
   },
   "outputs": [],
   "source": [
    "# Time series split function (Expanding Window)\n",
    "def time_series_split(df, n_splits=N_SPLITS, test_size=0.2):\n",
    "    df = df.sort_values('timestamp')\n",
    "    test_split_index = int(len(df) * (1 - test_size))\n",
    "    train_val_df = df.iloc[:test_split_index]\n",
    "    test_df = df.iloc[test_split_index:]\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    splits = [(train_val_df.iloc[train_index], train_val_df.iloc[val_index]) for train_index, val_index in tscv.split(train_val_df)]\n",
    "    return splits, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oS3gb2T6Po6"
   },
   "outputs": [],
   "source": [
    "# Sequence creation for univariate time series\n",
    "def create_sequences(df, lookback=LOOKBACK, horizon=HORIZON):\n",
    "    X, y, anomaly, cell_id = [], [], [], []\n",
    "\n",
    "    # Loop through each unique cell in the dataset\n",
    "    for cell in df['cell'].unique():\n",
    "        # Filter the dataframe for the current cell only\n",
    "        cell_df = df[df['cell'] == cell]\n",
    "\n",
    "        # Generate sequences within this cell's data\n",
    "        for i in range(lookback, len(cell_df) - horizon + 1):\n",
    "            # Lookback sequence for minRSSI only (univariate)\n",
    "            X_seq = cell_df.iloc[i - lookback:i][['minRSSI']].values\n",
    "            # Target horizon sequence for minRSSI\n",
    "            y_seq = cell_df.iloc[i:i + horizon]['minRSSI'].values\n",
    "            # Anomaly sequences for later evaluation\n",
    "            anomaly_seq = cell_df.iloc[i:i + horizon]['anomaly'].values\n",
    "            # Cell ID for each sequence\n",
    "            cell_seq = cell_df.iloc[i:i + horizon]['cell'].values\n",
    "\n",
    "            # Append sequences to output lists\n",
    "            X.append(X_seq)\n",
    "            y.append(y_seq)\n",
    "            anomaly.append(anomaly_seq)\n",
    "            cell_id.append(cell_seq)\n",
    "\n",
    "    # Convert lists to numpy arrays for model input\n",
    "    return np.array(X), np.array(y), np.array(anomaly), np.array(cell_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHHpMuv8MBqO"
   },
   "outputs": [],
   "source": [
    "def tune_hp_hyperband_lstm(X_train, y_train, max_epochs=EPOCHS):\n",
    "\n",
    "    def build_tunable_lstm(hp):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(X_train.shape[1], 1)))\n",
    "\n",
    "        # Add LSTM layers with recurrent dropout\n",
    "        for i in range(hp.Int('num_lstm_layers', min_value=1, max_value=4)):\n",
    "            model.add(LSTM(\n",
    "                units=hp.Choice('units', values=[32, 64, 128, 256]),\n",
    "                activation=hp.Choice('activation', values=['relu', 'tanh']),\n",
    "                return_sequences=True if i < hp.get('num_lstm_layers') - 1 else False,\n",
    "                kernel_regularizer=l2(hp.Choice('l2_regularizer', values=[1e-2, 1e-3, 1e-4])),\n",
    "                recurrent_dropout=hp.Float(f'recurrent_dropout_{i}', min_value=0.2, max_value=0.5, step=0.1)\n",
    "            ))\n",
    "\n",
    "        # Add dense layers\n",
    "        for j in range(hp.Int('num_dense_layers', min_value=1, max_value=3)):\n",
    "            model.add(Dense(units=hp.Choice(f'dense_units_{j}', values=[32, 64, 128, 256])))\n",
    "\n",
    "            # Optional dense layer dropout\n",
    "            if hp.Boolean(f'use_dense_dropout_{j}'):\n",
    "                dense_dropout_rate = hp.Float(f'dense_dropout_rate_{j}', min_value=0.1, max_value=0.3, step=0.1)\n",
    "                model.add(Dropout(dense_dropout_rate))\n",
    "\n",
    "        model.add(Dense(HORIZON))\n",
    "\n",
    "        # Compile model with fixed optimizer (Adam) and tunable loss function\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=hp.Choice('loss', values=['mse', 'mae'])\n",
    "        )\n",
    "\n",
    "        # Define batch size as a tunable hyperparameter\n",
    "        batch_size = hp.Choice('batch_size', [16, 32, 64, 128])\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Hyperband tuner instance\n",
    "    tuner = kt.Hyperband(\n",
    "        hypermodel=build_tunable_lstm,\n",
    "        objective='val_loss',\n",
    "        max_epochs=max_epochs,\n",
    "        factor=3,\n",
    "        directory='/content/drive/MyDrive/Thesis/Thesis/lstm/univar_tuning',\n",
    "        project_name='lstm_tuning'\n",
    "    )\n",
    "\n",
    "    # Fit Hyperband tuner to training data\n",
    "    tuner.search(X_train, y_train,\n",
    "                 epochs=max_epochs,\n",
    "                 batch_size=hp.Choice('batch_size', values=[16, 32, 64, 128]),\n",
    "                 validation_split=0.2,\n",
    "                 verbose=1)\n",
    "\n",
    "    # Get best model and hyperparameters\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_params = tuner.get_best_hyperparameters(num_trials=1)[0].values\n",
    "\n",
    "    return best_model, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isvcolF8_kWu"
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 4269,
     "status": "ok",
     "timestamp": 1731327523582,
     "user": {
      "displayName": "Jakob",
      "userId": "17712631153690750491"
     },
     "user_tz": -60
    },
    "id": "vW_sPK1f_6Xs",
    "outputId": "4c81de88-2034-4b41-e858-991c8e64bd86"
   },
   "outputs": [],
   "source": [
    "imp_folder = os.getenv(\"DATA_PATH\", \"./default_data_path/\")\n",
    "exp_folder = os.getenv(\"MODEL_PATH\", \"./default_model_path/\")\n",
    "\n",
    "df = pd.read_csv(imp_folder + 'cell_undersampled_1.csv')\n",
    "df = df[['timestamp', 'cell', 'minRSSI', 'anomaly']]\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2346,
     "status": "ok",
     "timestamp": 1731327525922,
     "user": {
      "displayName": "Jakob",
      "userId": "17712631153690750491"
     },
     "user_tz": -60
    },
    "id": "LunGFPbs_ZEE",
    "outputId": "d3037f0d-f924-4e23-fef9-4ef1b76cb8c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1:\n",
      "  Train set shape: (151460, 4)\n",
      "  Validation set shape: (151459, 4)\n",
      "Split 2:\n",
      "  Train set shape: (302919, 4)\n",
      "  Validation set shape: (151459, 4)\n",
      "Split 3:\n",
      "  Train set shape: (454378, 4)\n",
      "  Validation set shape: (151459, 4)\n",
      "Split 4:\n",
      "  Train set shape: (605837, 4)\n",
      "  Validation set shape: (151459, 4)\n",
      "Test set shape: (189324, 4)\n"
     ]
    }
   ],
   "source": [
    "splits, test_set = time_series_split(df, 4)\n",
    "\n",
    "for i, (train, val) in enumerate(splits):\n",
    "    print(f\"Split {i + 1}:\")\n",
    "    print(f\"  Train set shape: {train.shape}\")\n",
    "    print(f\"  Validation set shape: {val.shape}\")\n",
    "\n",
    "print(f\"Test set shape: {test_set.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SueCamVvA7Wp"
   },
   "source": [
    "# Run the tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6qXfY5FjA8aH",
    "outputId": "319f0eb8-a2e3-45b0-dfce-f8b884d70008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28 Complete [00h 13m 09s]\n",
      "val_loss: 0.7287741899490356\n",
      "\n",
      "Best val_loss So Far: 0.5271056890487671\n",
      "Total elapsed time: 05h 54m 16s\n",
      "\n",
      "Search: Running Trial #29\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "3                 |2                 |num_lstm_layers\n",
      "192               |32                |units\n",
      "tanh              |relu              |activation\n",
      "0.0004747         |0.0001045         |l2_regularizer\n",
      "0.4               |0.2               |recurrent_dropout_0\n",
      "1                 |3                 |num_dense_layers\n",
      "128               |256               |dense_units_0\n",
      "True              |False             |use_dense_dropout_0\n",
      "rmsprop           |adam              |optimizer\n",
      "mae               |mae               |loss\n",
      "0.4               |0.4               |recurrent_dropout_1\n",
      "0.2               |0.4               |recurrent_dropout_2\n",
      "0.2               |0.3               |recurrent_dropout_3\n",
      "128               |32                |dense_units_1\n",
      "True              |True              |use_dense_dropout_1\n",
      "128               |128               |dense_units_2\n",
      "False             |False             |use_dense_dropout_2\n",
      "0.2               |0.1               |dense_dropout_rate_0\n",
      "0.1               |0.2               |dense_dropout_rate_2\n",
      "0.2               |0.2               |dense_dropout_rate_1\n",
      "10                |10                |tuner/epochs\n",
      "0                 |4                 |tuner/initial_epoch\n",
      "0                 |2                 |tuner/bracket\n",
      "0                 |2                 |tuner/round\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m3457/3457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 116ms/step - loss: 0.5020 - val_loss: 0.5457\n",
      "Epoch 2/10\n",
      "\u001b[1m3457/3457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 117ms/step - loss: 0.4459 - val_loss: 0.5421\n",
      "Epoch 3/10\n",
      "\u001b[1m3457/3457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 118ms/step - loss: 0.4370 - val_loss: 0.5405\n",
      "Epoch 4/10\n",
      "\u001b[1m3457/3457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 117ms/step - loss: 0.4327 - val_loss: 0.5369\n",
      "Epoch 5/10\n",
      "\u001b[1m3457/3457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 116ms/step - loss: 0.4289 - val_loss: 0.5415\n",
      "Epoch 6/10\n",
      "\u001b[1m3457/3457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 119ms/step - loss: 0.4261 - val_loss: 0.5411\n",
      "Epoch 7/10\n",
      "\u001b[1m3457/3457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 118ms/step - loss: 0.4244 - val_loss: 0.5354\n",
      "Epoch 8/10\n",
      "\u001b[1m3457/3457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 118ms/step - loss: 0.4231 - val_loss: 0.5403\n",
      "Epoch 9/10\n"
     ]
    }
   ],
   "source": [
    "all_best_params = []\n",
    "\n",
    "for i, (train_set, val_set) in enumerate(splits):\n",
    "    print(f\"Processing split {i + 1}/{N_SPLITS}\")\n",
    "\n",
    "    # Preprocess the data (fit scaler on train, transform on train and val)\n",
    "    train_set['minRSSI'] = scaler.fit_transform(train_set[['minRSSI']])\n",
    "    val_set['minRSSI'] = scaler.transform(val_set[['minRSSI']])\n",
    "\n",
    "    # Create sequences for train and validation sets\n",
    "    X_train, y_train, _, _ = create_sequences(train_set)\n",
    "    X_val, y_val, _, _ = create_sequences(val_set)\n",
    "\n",
    "    # Reshape X for 1D CNN (samples, timesteps, features)\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "\n",
    "    # Tune and train the model\n",
    "    best_model, best_params = tune_hp_hyperband_lstm(X_train, y_train)\n",
    "    all_best_params.append(best_params)\n",
    "\n",
    "    # Evaluate the model on validation set\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "    print(f\"Split {i + 1} Results:\")\n",
    "    print(f\"  Best parameters: {best_params}\")\n",
    "    print(f\"  Validation RMSE: {val_rmse}\")\n",
    "    print(f\"  Validation MAE: {val_mae}\")\n",
    "\n",
    "    # Save the best model for the last split only\n",
    "    if i == len(splits) - 1:\n",
    "        best_model.save(os.path.join(exp_folder, 'best_lstm_model.keras'))\n",
    "        print(\"Saved the best model from the last split.\")\n",
    "\n",
    "print()\n",
    "print(\"All best hyperparameters across splits:\", all_best_params)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
