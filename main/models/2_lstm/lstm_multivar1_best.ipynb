{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22419,
     "status": "ok",
     "timestamp": 1733074037277,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "dD5zQDdqMWLa",
    "outputId": "97aca74a-8972-46e0-c3b2-5b440b0bca1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4624,
     "status": "ok",
     "timestamp": 1733074041887,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "dazA9FqwLJie"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import utils\n",
    "\n",
    "utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1733074041889,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "VjTCMXai53r8"
   },
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "LOOKBACK = 24\n",
    "HORIZON = 24\n",
    "N_SPLITS = 4\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "model_name = 'lstm_multi1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isvcolF8_kWu"
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4243,
     "status": "ok",
     "timestamp": 1733074046108,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "vW_sPK1f_6Xs",
    "outputId": "c75ab5bb-95fd-4ef9-a0f7-a9e6c67439da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(933661, 24)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'cell', 'bts', 'antenna', 'carrier', 'minRSSI',\n",
       "       'pageSessions', 'ULvol', 'sessionDur', 'blocks', 'AnomalyDay',\n",
       "       'anomaly', 'noise', 'Height', 'Azimuth', 'SectorsPerBts', 'NearbyBts',\n",
       "       'Dist2Coast', 'ClusterId', 'CellsPerBts', 'OverallPageSessions',\n",
       "       'OverallULvol', 'OverallSessionDur', 'OverallBlocks'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_folder = os.getenv(\"DATA_PATH\", \"./default_data_path/\")\n",
    "exp_folder = os.getenv(\"MODEL_PATH\", \"./default_model_path/\")\n",
    "\n",
    "df = pd.read_csv(imp_folder + 'cell_multivar.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1733074046108,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "OYeeCcNIUd_F"
   },
   "outputs": [],
   "source": [
    "temporal_X = ['pageSessions', 'ULvol', 'sessionDur']\n",
    "static_X = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPkJYqv65-8O"
   },
   "source": [
    "# Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1733074046108,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "ldcRlogr5843"
   },
   "outputs": [],
   "source": [
    "# Time series split function (Expanding Window)\n",
    "def time_series_split(df, n_splits=N_SPLITS, test_size=0.2):\n",
    "    df = df.sort_values('timestamp')\n",
    "    test_split_index = int(len(df) * (1 - test_size))\n",
    "    train_val_df = df.iloc[:test_split_index]\n",
    "    test_df = df.iloc[test_split_index:]\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    splits = [(train_val_df.iloc[train_index], train_val_df.iloc[val_index]) for train_index, val_index in tscv.split(train_val_df)]\n",
    "    return splits, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1733074046108,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "6oS3gb2T6Po6"
   },
   "outputs": [],
   "source": [
    "# Sequence creation for multivariate time series\n",
    "def create_sequences(df, lookback=LOOKBACK, horizon=HORIZON, static_X=static_X, temporal_X=temporal_X):\n",
    "    X, y, anomaly, cell_id = [], [], [], []\n",
    "\n",
    "    # Loop through each unique cell in the dataset\n",
    "    for cell in df['cell'].unique():\n",
    "        # Filter the dataframe for the current cell only\n",
    "        cell_df = df[df['cell'] == cell]\n",
    "\n",
    "        # Generate sequences within this cell's data\n",
    "        for i in range(lookback, len(cell_df) - horizon + 1):\n",
    "            # Lookback sequences with time-variant features\n",
    "            X_seq = cell_df.iloc[i - lookback:i][['minRSSI'] + temporal_X].values\n",
    "\n",
    "            # Repeat static features across lookback window and concatenate to time-variant features\n",
    "            static_seq = cell_df.iloc[i][static_X].values  # Static features for this cell at a single timestep\n",
    "            static_seq = np.tile(static_seq, (lookback, 1))  # Repeat to match lookback window length\n",
    "\n",
    "            # Concatenate time-variant and time-invariant features\n",
    "            X_combined = np.concatenate([X_seq, static_seq], axis=1)\n",
    "\n",
    "            # Target horizon sequence\n",
    "            y_seq = cell_df.iloc[i:i + horizon]['minRSSI'].values\n",
    "            # Anomaly sequences for later evaluation\n",
    "            anomaly_seq = cell_df.iloc[i:i + horizon]['anomaly'].values\n",
    "            # Cell ID for each sequence\n",
    "            cell_seq = cell_df.iloc[i:i + horizon]['cell'].values\n",
    "\n",
    "            # Append sequences to output lists\n",
    "            X.append(X_combined)\n",
    "            y.append(y_seq)\n",
    "            anomaly.append(anomaly_seq)\n",
    "            cell_id.append(cell_seq)\n",
    "\n",
    "    # Convert lists to numpy arrays for model input\n",
    "    return np.array(X), np.array(y), np.array(anomaly), np.array(cell_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1733074046109,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "FSmSO9zRSSu-"
   },
   "outputs": [],
   "source": [
    "def scale_data_split(train_df, val_df, temporal_features=temporal_X, static_features=static_X):\n",
    "    scaler_temporal = StandardScaler()\n",
    "    scaler_static = MinMaxScaler()\n",
    "    scaler_target = StandardScaler()\n",
    "\n",
    "    # Scale time-variant features\n",
    "    if temporal_features:\n",
    "        train_df[temporal_features] = scaler_temporal.fit_transform(train_df[temporal_features])\n",
    "        val_df[temporal_features] = scaler_temporal.transform(val_df[temporal_features])\n",
    "\n",
    "    # Scale time-invariant features\n",
    "    if static_features:\n",
    "        train_df[static_features] = scaler_static.fit_transform(train_df[static_features])\n",
    "        val_df[static_features] = scaler_static.transform(val_df[static_features])\n",
    "\n",
    "    # Scale minRSSI separately (target variable)\n",
    "    train_df['minRSSI'] = scaler_target.fit_transform(train_df[['minRSSI']])\n",
    "    val_df['minRSSI'] = scaler_target.transform(val_df[['minRSSI']])\n",
    "\n",
    "    return train_df, val_df, scaler_target, scaler_temporal, scaler_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1733074046109,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "rHHpMuv8MBqO"
   },
   "outputs": [],
   "source": [
    "def build_lstm(lookback, horizon, n_features):\n",
    "    model = Sequential([\n",
    "        Input(shape=(lookback, n_features)),\n",
    "\n",
    "        LSTM(\n",
    "            units=10,\n",
    "            activation='relu',\n",
    "            return_sequences=False,\n",
    "            kernel_regularizer=l2(0.001),\n",
    "            recurrent_dropout=0.3\n",
    "        ),\n",
    "\n",
    "        Dense(96),\n",
    "        Dense(horizon)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1733074046110,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "IGmfE83aRyHC"
   },
   "outputs": [],
   "source": [
    "def train_validate(splits, lookback, horizon):\n",
    "    model = None\n",
    "    results = []\n",
    "    scalers = {}\n",
    "    total_training_time = 0\n",
    "\n",
    "    for i, (train_df, val_df) in enumerate(splits):\n",
    "        print(f\"\\nProcessing Split {i + 1}/{len(splits)}\")\n",
    "\n",
    "        # Scale the current split\n",
    "        scaled_train, scaled_val, scaler_target, scaler_temporal, scaler_static = scale_data_split(train_df.copy(), val_df.copy())\n",
    "\n",
    "        scalers = {\n",
    "            'scaler_target': scaler_target,\n",
    "            'scaler_temporal': scaler_temporal,\n",
    "            'scaler_static': scaler_static\n",
    "        }\n",
    "\n",
    "        # Create sequences\n",
    "        X_train, y_train, _, _ = create_sequences(scaled_train, LOOKBACK, HORIZON)\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        X_val, y_val, val_anomalies, _ = create_sequences(scaled_val, LOOKBACK, HORIZON)\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "\n",
    "        # Convert to float32 to avoid data type issues\n",
    "        X_train, y_train = X_train.astype(np.float32), y_train.astype(np.float32)\n",
    "        X_val, y_val = X_val.astype(np.float32), y_val.astype(np.float32)\n",
    "\n",
    "        n_features = X_train.shape[2]\n",
    "\n",
    "        # Create a fresh model for each split\n",
    "        model = build_lstm(LOOKBACK, HORIZON, n_features)\n",
    "\n",
    "        # Early stopping callback\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        # Start timer\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # End timer\n",
    "        split_training_time = time.time() - start_time\n",
    "        total_training_time += split_training_time\n",
    "\n",
    "        # Evaluation\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_val_og, y_pred_og = scalers['scaler_target'].inverse_transform(y_val), scalers['scaler_target'].inverse_transform(y_pred)\n",
    "\n",
    "        mae, rmse = mean_absolute_error(y_val_og, y_pred_og), np.sqrt(mean_squared_error(y_val_og, y_pred_og))\n",
    "        anom_mae, anom_rmse = [], []\n",
    "\n",
    "        for step in range(horizon):\n",
    "            step_anomaly_mask = val_anomalies[:, step] == 1\n",
    "            if np.any(step_anomaly_mask):\n",
    "                anom_mae.append(mean_absolute_error(y_val_og[step_anomaly_mask, step], y_pred_og[step_anomaly_mask, step]))\n",
    "                anom_rmse.append(np.sqrt(mean_squared_error(y_val_og[step_anomaly_mask, step], y_pred_og[step_anomaly_mask, step])))\n",
    "            else:\n",
    "                anom_mae.append(np.nan)\n",
    "                anom_rmse.append(np.nan)\n",
    "\n",
    "        results.append({'split': i + 1,\n",
    "                        'Overall_MAE': mae,\n",
    "                        'Overall_RMSE': rmse,\n",
    "                        'Anom_MAE': np.nanmean(anom_mae),\n",
    "                        'Anom_RMSE': np.nanmean(anom_rmse)})\n",
    "        print(f\"Split {i + 1} - Overall MAE: {mae:.4f}, Overall RMSE: {rmse:.4f}, \"\n",
    "              f\"Overall Anomaly MAE: {np.nanmean(anom_mae):.4f}, Overall Anomaly RMSE: {np.nanmean(anom_rmse):.4f}\")\n",
    "\n",
    "     # Convert total training time to minutes and seconds format\n",
    "    minutes, seconds = divmod(total_training_time, 60)\n",
    "\n",
    "    # Aggregate results across splits\n",
    "    avg_overall_mae = np.mean([res['Overall_MAE'] for res in results])\n",
    "    avg_overall_rmse = np.mean([res['Overall_RMSE'] for res in results])\n",
    "    avg_overall_anom_mae = np.nanmean([res['Anom_MAE'] for res in results])\n",
    "    avg_overall_anom_rmse = np.nanmean([res['Anom_RMSE'] for res in results])\n",
    "\n",
    "    summary_results = {\n",
    "        'Average Overall MAE': avg_overall_mae,\n",
    "        'Average Overall RMSE': avg_overall_rmse,\n",
    "        'Average Overall Anomaly MAE': avg_overall_anom_mae,\n",
    "        'Average Overall Anomaly RMSE': avg_overall_anom_rmse,\n",
    "        'Total Training Time': f\"{int(minutes)}m {int(seconds)}s\"\n",
    "    }\n",
    "\n",
    "    return summary_results, model, scalers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1733074046110,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "_PDYx_uaR4rs"
   },
   "outputs": [],
   "source": [
    "def save_model(model, model_path=exp_folder):\n",
    "    \"\"\"Save the trained model and weights to a file.\"\"\"\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1733074046110,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "s-h3icoGR-Uh"
   },
   "outputs": [],
   "source": [
    "def scale_test_data(test_df, scaler_target, scaler_temporal, scaler_static, temporal_features=temporal_X, static_features=static_X):\n",
    "    \"\"\"Scale the test data using the same scalers as for the training/validation set.\"\"\"\n",
    "    # Scale minRSSI separately (target variable)\n",
    "    test_df['minRSSI'] = scaler_target.transform(test_df[['minRSSI']])\n",
    "\n",
    "    # Scale other time-variant features\n",
    "    test_df[temporal_features] = scaler_temporal.transform(test_df[temporal_features])\n",
    "\n",
    "    if static_features:\n",
    "      test_df[static_features] = scaler_static.transform(test_df[static_features])\n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1733074046111,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "cx5fAMLFSA2W"
   },
   "outputs": [],
   "source": [
    "def evaluate_test(model, test_df, scalers, lookback=LOOKBACK, horizon=HORIZON):\n",
    "    \"\"\"Evaluate the trained model on the test set and calculate MAE, RMSE for all steps.\"\"\"\n",
    "    print(\"Preparing Test Data...\")\n",
    "\n",
    "    # Scale the current split\n",
    "    scaled_test = scale_test_data(test_df.copy(), scalers['scaler_target'], scalers['scaler_temporal'], scalers['scaler_static'])\n",
    "\n",
    "    # Prepare test sequences\n",
    "    X_test, y_test, test_anomalies, test_cells = create_sequences(scaled_test, lookback, horizon)\n",
    "\n",
    "    # Convert to float32\n",
    "    X_test, y_test = X_test.astype(np.float32), y_test.astype(np.float32)\n",
    "\n",
    "    print('X_test, y_test shapes:', X_test.shape, y_test.shape)\n",
    "\n",
    "    print(\"\\nPredicting...\")\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Inverse transform predictions and actual values\n",
    "    y_test_original = scalers['scaler_target'].inverse_transform(y_test).reshape(-1, horizon)\n",
    "    y_pred_original = scalers['scaler_target'].inverse_transform(y_pred).reshape(-1, horizon)\n",
    "\n",
    "    # Calculate per-step MAE and RMSE\n",
    "    per_step_mae, per_step_rmse = [], []\n",
    "    per_step_anom_mae, per_step_anom_rmse = [], []\n",
    "\n",
    "    print(\"\\nEvaluating...\")\n",
    "    for step in range(horizon):\n",
    "        # Calculate general per-step metrics (MAE, RMSE)\n",
    "        mae_step = mean_absolute_error(y_test_original[:, step], y_pred_original[:, step])\n",
    "        rmse_step = np.sqrt(mean_squared_error(y_test_original[:, step], y_pred_original[:, step]))\n",
    "        per_step_mae.append(mae_step)\n",
    "        per_step_rmse.append(rmse_step)\n",
    "\n",
    "        # Anomaly-specific metrics (only considering values where anomaly == 1)\n",
    "        step_anomaly_mask = test_anomalies[:, step] == 1\n",
    "        if np.any(step_anomaly_mask):\n",
    "            anom_mae_step = mean_absolute_error(y_test_original[step_anomaly_mask, step], y_pred_original[step_anomaly_mask, step])\n",
    "            anom_rmse_step = np.sqrt(mean_squared_error(y_test_original[step_anomaly_mask, step], y_pred_original[step_anomaly_mask, step]))\n",
    "        else:\n",
    "            anom_mae_step, anom_rmse_step = np.nan, np.nan\n",
    "\n",
    "        per_step_anom_mae.append(anom_mae_step)\n",
    "        per_step_anom_rmse.append(anom_rmse_step)\n",
    "\n",
    "    # Calculate overall MAE and RMSE across all steps\n",
    "    overall_mae = np.mean(per_step_mae)\n",
    "    overall_rmse = np.mean(per_step_rmse)\n",
    "    overall_anom_mae = np.nanmean(per_step_anom_mae)\n",
    "    overall_anom_rmse = np.nanmean(per_step_anom_rmse)\n",
    "\n",
    "    print(f\"Test MAE: {overall_mae:.4f}, Test RMSE: {overall_rmse:.4f}\")\n",
    "    print(f\"Test Anomaly MAE: {overall_anom_mae:.4f}, Test Anomaly RMSE: {overall_anom_rmse:.4f}\")\n",
    "\n",
    "    # Create the results dictionary for evaluation metrics\n",
    "    results = {\n",
    "        'MAE_per_step': per_step_mae,\n",
    "        'RMSE_per_step': per_step_rmse,\n",
    "        'Anom_MAE_per_step': per_step_anom_mae,\n",
    "        'Anom_RMSE_per_step': per_step_anom_rmse,\n",
    "        'Overall_MAE': overall_mae,\n",
    "        'Overall_RMSE': overall_rmse,\n",
    "        'Overall_Anomaly_MAE': overall_anom_mae,\n",
    "        'Overall_Anomaly_RMSE': overall_anom_rmse\n",
    "    }\n",
    "\n",
    "    # Create a dictionary for predictions (actual vs predicted) for plotting or further analysis\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(y_test_original.shape[0]):  # Iterate over each sample (cell)\n",
    "        row = {'cell_id': test_cells[i][0]}\n",
    "\n",
    "        for step in range(horizon):\n",
    "            row[f'actual_{step+1}'] = y_test_original[i, step]\n",
    "            row[f'predicted_{step+1}'] = y_pred_original[i, step]\n",
    "            row[f'anomaly_{step+1}'] = test_anomalies[i, step]\n",
    "\n",
    "        predictions.append(row)\n",
    "\n",
    "    # Create a DataFrame for predictions to save or plot later\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "    # Return the results dictionary (evaluation metrics) and predictions DataFrame\n",
    "    return results, predictions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1733074046111,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "gcoOLb2USHFm"
   },
   "outputs": [],
   "source": [
    "def plot_predictions(pred_df_path, cell_id, horizon):\n",
    "    # Load the predictions DataFrame\n",
    "    pred_df = pd.read_csv(pred_df_path)\n",
    "\n",
    "    # Filter the DataFrame for the specified cell_id\n",
    "    cell_data = pred_df[pred_df['cell_id'] == cell_id]\n",
    "\n",
    "    # Extract the actual, predicted values and anomalies for the specified horizon\n",
    "    actual_col = f'actual_{horizon}'\n",
    "    predicted_col = f'predicted_{horizon}'\n",
    "    anomaly_col = f'anomaly_{horizon}'\n",
    "\n",
    "    actual_values = cell_data[actual_col].values\n",
    "    predicted_values = cell_data[predicted_col].values\n",
    "    anomalies = cell_data[anomaly_col].values\n",
    "\n",
    "    # Plot the actual and predicted values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(actual_values, label='Actual', color='blue')\n",
    "    plt.plot(predicted_values, label='Predicted', color='orange')\n",
    "\n",
    "    # Highlight anomalies with red dots\n",
    "    anomaly_indices = anomalies == 1\n",
    "    plt.scatter(np.arange(len(actual_values))[anomaly_indices],\n",
    "                actual_values[anomaly_indices], color='red', label='Anomaly',\n",
    "                marker='o', s=50, edgecolors='k', zorder=5)\n",
    "\n",
    "    if horizon == 1:\n",
    "        time_step_desc = \"30 minutes\"\n",
    "    else:\n",
    "        time_step_desc = f\"{horizon * 0.5} hours\"\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.title(f'Actual vs Predicted for Cell {cell_id} at Step {horizon} ({time_step_desc} Ahead)')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('minRSSI')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SueCamVvA7Wp"
   },
   "source": [
    "# Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1149,
     "status": "ok",
     "timestamp": 1733074047216,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "QOKsYCyqX5cT",
    "outputId": "a59d615f-2c61-404d-cdfc-b64a8fa4fdd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1:\n",
      "  Train set shape: (149388, 24)\n",
      "  Validation set shape: (149385, 24)\n",
      "Split 2:\n",
      "  Train set shape: (298773, 24)\n",
      "  Validation set shape: (149385, 24)\n",
      "Split 3:\n",
      "  Train set shape: (448158, 24)\n",
      "  Validation set shape: (149385, 24)\n",
      "Split 4:\n",
      "  Train set shape: (597543, 24)\n",
      "  Validation set shape: (149385, 24)\n",
      "Test set shape: (186733, 24)\n"
     ]
    }
   ],
   "source": [
    "splits, test_set = time_series_split(df, N_SPLITS)\n",
    "\n",
    "for i, (train, val) in enumerate(splits):\n",
    "    print(f\"Split {i + 1}:\")\n",
    "    print(f\"  Train set shape: {train.shape}\")\n",
    "    print(f\"  Validation set shape: {val.shape}\")\n",
    "\n",
    "print(f\"Test set shape: {test_set.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOc8oBFLcwnm",
    "outputId": "c0bfacd1-99bd-49c8-c95f-2aecf9a8f75c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Split 1/4\n",
      "X_train shape: (136369, 24, 4), y_train shape: (136369, 24)\n",
      "X_val shape: (136366, 24, 4), y_val shape: (136366, 24)\n",
      "Epoch 1/20\n",
      "\u001b[1m8524/8524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 6ms/step - loss: 0.4967 - val_loss: 0.3311\n",
      "Epoch 2/20\n",
      "\u001b[1m8524/8524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - loss: 0.4579 - val_loss: 0.3268\n",
      "Epoch 3/20\n",
      "\u001b[1m8524/8524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - loss: 0.4479 - val_loss: 0.3193\n",
      "Epoch 4/20\n",
      "\u001b[1m8524/8524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - loss: 0.4424 - val_loss: 0.3182\n",
      "Epoch 5/20\n",
      "\u001b[1m8524/8524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - loss: 0.4403 - val_loss: 0.3170\n",
      "Epoch 6/20\n",
      "\u001b[1m8524/8524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - loss: 0.4384 - val_loss: 0.3173\n",
      "Epoch 7/20\n",
      "\u001b[1m8524/8524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - loss: 0.4370 - val_loss: 0.3177\n",
      "Epoch 8/20\n",
      "\u001b[1m8524/8524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - loss: 0.4363 - val_loss: 0.3171\n",
      "Epoch 9/20\n",
      "\u001b[1m8524/8524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - loss: 0.4348 - val_loss: 0.3155\n",
      "Epoch 10/20\n",
      "\u001b[1m8524/8524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - loss: 0.4345 - val_loss: 0.3183\n",
      "Epoch 11/20\n",
      "\u001b[1m8524/8524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - loss: 0.4341 - val_loss: 0.3155\n",
      "Epoch 12/20\n",
      "\u001b[1m8524/8524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - loss: 0.4341 - val_loss: 0.3164\n",
      "Epoch 13/20\n",
      "\u001b[1m8524/8524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - loss: 0.4338 - val_loss: 0.3183\n",
      "Epoch 14/20\n",
      "\u001b[1m8524/8524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - loss: 0.4332 - val_loss: 0.3182\n",
      "Epoch 15/20\n",
      "\u001b[1m8524/8524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - loss: 0.4325 - val_loss: 0.3203\n",
      "Epoch 16/20\n",
      "\u001b[1m8524/8524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 6ms/step - loss: 0.4326 - val_loss: 0.3204\n",
      "\u001b[1m4262/4262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step\n",
      "Split 1 - Overall MAE: 1.6901, Overall RMSE: 3.1586, Overall Anomaly MAE: 13.3218, Overall Anomaly RMSE: 14.0795\n",
      "\n",
      "Processing Split 2/4\n",
      "X_train shape: (285754, 24, 4), y_train shape: (285754, 24)\n",
      "X_val shape: (136366, 24, 4), y_val shape: (136366, 24)\n",
      "Epoch 1/20\n",
      "\u001b[1m17860/17860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 5ms/step - loss: 0.4325 - val_loss: 0.1728\n",
      "Epoch 2/20\n",
      "\u001b[1m17860/17860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 5ms/step - loss: 0.4002 - val_loss: 0.1665\n",
      "Epoch 3/20\n",
      "\u001b[1m17860/17860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 4ms/step - loss: 0.3949 - val_loss: 0.1676\n",
      "Epoch 4/20\n",
      "\u001b[1m17860/17860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 4ms/step - loss: 0.3926 - val_loss: 0.1636\n",
      "Epoch 5/20\n",
      "\u001b[1m17860/17860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 4ms/step - loss: 0.3917 - val_loss: 0.1566\n",
      "Epoch 6/20\n",
      "\u001b[1m17860/17860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 4ms/step - loss: 0.3904 - val_loss: 0.1549\n",
      "Epoch 7/20\n",
      "\u001b[1m17860/17860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 4ms/step - loss: 0.3891 - val_loss: 0.1557\n",
      "Epoch 8/20\n",
      "\u001b[1m17860/17860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 4ms/step - loss: 0.3880 - val_loss: 0.1549\n",
      "Epoch 9/20\n",
      "\u001b[1m17860/17860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 4ms/step - loss: 0.3874 - val_loss: 0.1506\n",
      "Epoch 10/20\n",
      "\u001b[1m17860/17860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 4ms/step - loss: 0.3869 - val_loss: 0.1521\n",
      "Epoch 11/20\n",
      "\u001b[1m17860/17860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 4ms/step - loss: 0.3861 - val_loss: 0.1557\n",
      "Epoch 12/20\n",
      "\u001b[1m17860/17860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 4ms/step - loss: 0.3854 - val_loss: 0.1539\n",
      "Epoch 13/20\n",
      "\u001b[1m17860/17860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 4ms/step - loss: 0.3847 - val_loss: 0.1533\n",
      "Epoch 14/20\n",
      "\u001b[1m17860/17860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 4ms/step - loss: 0.3845 - val_loss: 0.1542\n",
      "\u001b[1m4262/4262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step\n",
      "Split 2 - Overall MAE: 0.7397, Overall RMSE: 1.6400, Overall Anomaly MAE: 20.9642, Overall Anomaly RMSE: 22.8119\n",
      "\n",
      "Processing Split 3/4\n",
      "X_train shape: (435139, 24, 4), y_train shape: (435139, 24)\n",
      "X_val shape: (136366, 24, 4), y_val shape: (136366, 24)\n",
      "Epoch 1/20\n",
      "\u001b[1m27197/27197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 4ms/step - loss: 0.3661 - val_loss: 0.2776\n",
      "Epoch 2/20\n",
      "\u001b[1m27197/27197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 4ms/step - loss: 0.3324 - val_loss: 0.2707\n",
      "Epoch 3/20\n",
      "\u001b[1m27197/27197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 4ms/step - loss: 0.3288 - val_loss: 0.2637\n",
      "Epoch 4/20\n",
      "\u001b[1m27197/27197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 4ms/step - loss: 0.3271 - val_loss: 0.2627\n",
      "Epoch 5/20\n",
      "\u001b[1m27197/27197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 4ms/step - loss: 0.3260 - val_loss: 0.2615\n",
      "Epoch 6/20\n",
      "\u001b[1m27197/27197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 4ms/step - loss: 0.3252 - val_loss: 0.2593\n",
      "Epoch 7/20\n",
      "\u001b[1m27197/27197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 4ms/step - loss: 0.3245 - val_loss: 0.2604\n",
      "Epoch 8/20\n",
      "\u001b[1m27197/27197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 4ms/step - loss: 0.3243 - val_loss: 0.2588\n",
      "Epoch 9/20\n",
      "\u001b[1m27197/27197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 4ms/step - loss: 0.3239 - val_loss: 0.2597\n",
      "Epoch 10/20\n",
      "\u001b[1m27197/27197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 4ms/step - loss: 0.3237 - val_loss: 0.2589\n",
      "Epoch 11/20\n",
      "\u001b[1m27197/27197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 4ms/step - loss: 0.3233 - val_loss: 0.2589\n",
      "Epoch 12/20\n",
      "\u001b[1m27197/27197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 4ms/step - loss: 0.3234 - val_loss: 0.2599\n",
      "Epoch 13/20\n",
      "\u001b[1m27197/27197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 4ms/step - loss: 0.3233 - val_loss: 0.2585\n",
      "Epoch 14/20\n",
      "\u001b[1m27197/27197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 4ms/step - loss: 0.3232 - val_loss: 0.2596\n",
      "Epoch 15/20\n",
      "\u001b[1m27197/27197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 4ms/step - loss: 0.3230 - val_loss: 0.2587\n",
      "Epoch 16/20\n",
      "\u001b[1m16986/27197\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 3ms/step - loss: 0.3228"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model across all splits\n",
    "summary_results, model, scalers = train_validate(splits, LOOKBACK, HORIZON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FX6zPdKqzA3"
   },
   "outputs": [],
   "source": [
    "print(\"\\nCross-Validation Results:\")\n",
    "for metric, value in summary_results.items():\n",
    "    try:\n",
    "        print(f\"{metric}: {float(value):.4f}\")\n",
    "    except ValueError:\n",
    "        print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IURBaI2DYEyR"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQmEXguZSfXY"
   },
   "outputs": [],
   "source": [
    "save_model(model, f'trained_{model_name}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CG0USCpRSau2"
   },
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LQTbEy7SaQo"
   },
   "outputs": [],
   "source": [
    "test_results, test_preds = evaluate_test(model, test_set, scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g75nsDbNS1KQ"
   },
   "outputs": [],
   "source": [
    "# Assuming test_results is your dictionary\n",
    "test_results_df = pd.DataFrame.from_dict(test_results)\n",
    "\n",
    "# Add the horizon_step column\n",
    "test_results_df['horizon_step'] = range(1, len(test_results_df) + 1)\n",
    "\n",
    "# Reorder columns to make horizon_step the first column\n",
    "cols = ['horizon_step'] + [col for col in test_results_df.columns if col != 'horizon_step']\n",
    "test_results_df = test_results_df[cols]\n",
    "\n",
    "# Save the test results and predicctions to CSV files\n",
    "test_results_df.to_csv(exp_folder + f'testresults_{model_name}.csv', index=False)\n",
    "test_preds.to_csv(exp_folder + f'test_preds_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_mDcIguTJ4E"
   },
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_counts = test_set[test_set['anomaly'] == 1].groupby('cell').size().reset_index(name='anomaly_count')\n",
    "top_3_cells = anomaly_counts.sort_values(by='anomaly_count', ascending=False).head(3)\n",
    "sel_cell = top_3_cells.iloc[1]['cell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XK53rmHkTHmT"
   },
   "outputs": [],
   "source": [
    "plot_predictions(exp_folder + f'test_preds_{model_name}.csv', sel_cell, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSdp3kmOTL9M"
   },
   "outputs": [],
   "source": [
    "plot_predictions(exp_folder + f'test_preds_{model_name}.csv', sel_cell, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "py_GABUVTNiI"
   },
   "outputs": [],
   "source": [
    "plot_predictions(exp_folder + f'test_preds_{model_name}.csv', sel_cell, 24)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "mPkJYqv65-8O"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
