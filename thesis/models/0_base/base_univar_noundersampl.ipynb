{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dazA9FqwLJie"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras import utils\n",
    "\n",
    "utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VjTCMXai53r8"
   },
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "LOOKBACK = 1\n",
    "HORIZON = 24\n",
    "N_SPLITS = 4\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "model_name='base_noundersampling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12868886, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>cell</th>\n",
       "      <th>bts</th>\n",
       "      <th>antenna</th>\n",
       "      <th>carrier</th>\n",
       "      <th>minRSSI</th>\n",
       "      <th>PageSessionTotal</th>\n",
       "      <th>ULvolMByte</th>\n",
       "      <th>AnomalyDay</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>noise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-01 02:30:00+00:00</td>\n",
       "      <td>997_0_0</td>\n",
       "      <td>997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-109.08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-01 02:30:00+00:00</td>\n",
       "      <td>580_0_1</td>\n",
       "      <td>580</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-103.58</td>\n",
       "      <td>67</td>\n",
       "      <td>0.150356</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-01 02:30:00+00:00</td>\n",
       "      <td>580_1_0</td>\n",
       "      <td>580</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-107.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0.352035</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-09-01 02:30:00+00:00</td>\n",
       "      <td>580_1_1</td>\n",
       "      <td>580</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-107.16</td>\n",
       "      <td>58</td>\n",
       "      <td>0.215608</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-09-01 02:30:00+00:00</td>\n",
       "      <td>580_2_0</td>\n",
       "      <td>580</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-106.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184871</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp     cell  bts  antenna  carrier  minRSSI  \\\n",
       "0  2023-09-01 02:30:00+00:00  997_0_0  997        0        0  -109.08   \n",
       "1  2023-09-01 02:30:00+00:00  580_0_1  580        0        1  -103.58   \n",
       "2  2023-09-01 02:30:00+00:00  580_1_0  580        1        0  -107.79   \n",
       "3  2023-09-01 02:30:00+00:00  580_1_1  580        1        1  -107.16   \n",
       "4  2023-09-01 02:30:00+00:00  580_2_0  580        2        0  -106.63   \n",
       "\n",
       "   PageSessionTotal  ULvolMByte  AnomalyDay  anomaly  noise  \n",
       "0                 0    0.000000           0        0      0  \n",
       "1                67    0.150356           0        0      0  \n",
       "2                 0    0.352035           0        0      0  \n",
       "3                58    0.215608           0        0      0  \n",
       "4                 0    0.184871           0        0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../../../config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "df = pd.read_csv(os.path.join(config['fulldataset'], 'cell_labeled.csv'))\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_X = []\n",
    "static_X = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPkJYqv65-8O"
   },
   "source": [
    "# Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ldcRlogr5843"
   },
   "outputs": [],
   "source": [
    "# Time series split function (Expanding Window)\n",
    "def time_series_split(df, n_splits=N_SPLITS, test_size=0.2):\n",
    "    df = df.sort_values('timestamp')\n",
    "    test_split_index = int(len(df) * (1 - test_size))\n",
    "    train_val_df = df.iloc[:test_split_index]\n",
    "    test_df = df.iloc[test_split_index:]\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    splits = [(train_val_df.iloc[train_index], train_val_df.iloc[val_index]) for train_index, val_index in tscv.split(train_val_df)]\n",
    "    return splits, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6oS3gb2T6Po6"
   },
   "outputs": [],
   "source": [
    "# Sequence creation for univariate time series\n",
    "def create_sequences(df, lookback=LOOKBACK, horizon=HORIZON):\n",
    "    X, y, anomaly, cell_id = [], [], [], []\n",
    "\n",
    "    # Loop through each unique cell in the dataset\n",
    "    for cell in df['cell'].unique():\n",
    "        # Filter the dataframe for the current cell only\n",
    "        cell_df = df[df['cell'] == cell]\n",
    "\n",
    "        # Generate sequences within this cell's data\n",
    "        for i in range(lookback, len(cell_df) - horizon + 1):\n",
    "            # Lookback sequence for minRSSI only (univariate)\n",
    "            X_seq = cell_df.iloc[i - lookback:i][['minRSSI']].values\n",
    "            # Target horizon sequence for minRSSI\n",
    "            y_seq = cell_df.iloc[i:i + horizon]['minRSSI'].values\n",
    "            # Anomaly sequences for later evaluation\n",
    "            anomaly_seq = cell_df.iloc[i:i + horizon]['anomaly'].values\n",
    "            # Cell ID for each sequence\n",
    "            cell_seq = cell_df.iloc[i:i + horizon]['cell'].values\n",
    "\n",
    "            # Append sequences to output lists\n",
    "            X.append(X_seq)\n",
    "            y.append(y_seq)\n",
    "            anomaly.append(anomaly_seq)\n",
    "            cell_id.append(cell_seq)\n",
    "\n",
    "    # Convert lists to numpy arrays for model input\n",
    "    return np.array(X), np.array(y), np.array(anomaly), np.array(cell_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data_split(train_df, val_df, temporal_features=temporal_X, static_features=static_X):\n",
    "    scaler_temporal = StandardScaler()\n",
    "    scaler_static = MinMaxScaler()\n",
    "    scaler_target = StandardScaler()\n",
    "\n",
    "    # Scale time-variant features\n",
    "    if temporal_features:\n",
    "        train_df[temporal_features] = scaler_temporal.fit_transform(train_df[temporal_features])\n",
    "        val_df[temporal_features] = scaler_temporal.transform(val_df[temporal_features])\n",
    "\n",
    "    # Scale time-invariant features\n",
    "    if static_features:\n",
    "        train_df[static_features] = scaler_static.fit_transform(train_df[static_features])\n",
    "        val_df[static_features] = scaler_static.transform(val_df[static_features])\n",
    "\n",
    "    # Scale minRSSI separately (target variable)\n",
    "    train_df['minRSSI'] = scaler_target.fit_transform(train_df[['minRSSI']])\n",
    "    val_df['minRSSI'] = scaler_target.transform(val_df[['minRSSI']])\n",
    "\n",
    "    return train_df, val_df, scaler_target, scaler_temporal, scaler_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MbE4f9PV8-fP"
   },
   "outputs": [],
   "source": [
    "def train_validate_dumb(splits, lookback=LOOKBACK, horizon=HORIZON):\n",
    "    results = []\n",
    "    scalers = {}\n",
    "    total_training_time = 0\n",
    "\n",
    "    for i, (train_df, val_df) in enumerate(splits):\n",
    "        print(f\"\\nProcessing Split {i + 1}/{len(splits)}\")\n",
    "\n",
    "        # Scale the current split\n",
    "        scaled_train, scaled_val, scaler_target, scaler_temporal, scaler_static = scale_data_split(train_df.copy(), val_df.copy())\n",
    "        scalers = {'scaler_target': scaler_target, 'scaler_temporal': scaler_temporal, 'scaler_static': scaler_static}\n",
    "\n",
    "        # Create sequences\n",
    "        X_train, y_train, _, _ = create_sequences(scaled_train, LOOKBACK, HORIZON)\n",
    "        X_val, y_val, val_anomalies, _ = create_sequences(scaled_val, LOOKBACK, HORIZON)\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "\n",
    "        # Start timer\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Dummy model predictions for validation (use the last minRSSI value)\n",
    "        y_pred_baseline = X_val[:, -1, 0].reshape(-1, 1).repeat(HORIZON, axis=1)\n",
    "\n",
    "        # End timer\n",
    "        split_training_time = time.time() - start_time\n",
    "        total_training_time += split_training_time\n",
    "\n",
    "        # Evaluate results\n",
    "        y_val_og = scalers['scaler_target'].inverse_transform(y_val)\n",
    "        y_pred_og = scalers['scaler_target'].inverse_transform(y_pred_baseline)\n",
    "\n",
    "        mae = mean_absolute_error(y_val_og, y_pred_og)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_og, y_pred_og))\n",
    "\n",
    "        # Anomaly-based evaluation\n",
    "        anom_mae, anom_rmse = [], []\n",
    "        for step in range(HORIZON):\n",
    "            step_anomaly_mask = val_anomalies[:, step] == 1\n",
    "            if np.any(step_anomaly_mask):\n",
    "                anom_mae.append(mean_absolute_error(y_val_og[step_anomaly_mask, step], y_pred_og[step_anomaly_mask, step]))\n",
    "                anom_rmse.append(np.sqrt(mean_squared_error(y_val_og[step_anomaly_mask, step], y_pred_og[step_anomaly_mask, step])))\n",
    "            else:\n",
    "                anom_mae.append(np.nan)\n",
    "                anom_rmse.append(np.nan)\n",
    "\n",
    "        results.append({'split': i + 1, 'Overall_MAE': mae, 'Overall_RMSE': rmse,\n",
    "                        'Anom_MAE': np.nanmean(anom_mae), 'Anom_RMSE': np.nanmean(anom_rmse)})\n",
    "\n",
    "        print(f\"Split {i + 1} - Overall MAE: {mae:.4f}, Overall RMSE: {rmse:.4f}, \"\n",
    "              f\"Anomaly MAE: {np.nanmean(anom_mae):.4f}, Anomaly RMSE: {np.nanmean(anom_rmse):.4f}\")\n",
    "\n",
    "    # Convert total training time to minutes and seconds format\n",
    "    minutes, seconds = divmod(total_training_time, 60)\n",
    "\n",
    "    # Aggregate results\n",
    "    avg_overall_mae = np.mean([res['Overall_MAE'] for res in results])\n",
    "    avg_overall_rmse = np.mean([res['Overall_RMSE'] for res in results])\n",
    "    avg_overall_anom_mae = np.nanmean([res['Anom_MAE'] for res in results])\n",
    "    avg_overall_anom_rmse = np.nanmean([res['Anom_RMSE'] for res in results])\n",
    "\n",
    "    summary_results = {\n",
    "        'Average Overall MAE': avg_overall_mae,\n",
    "        'Average Overall RMSE': avg_overall_rmse,\n",
    "        'Average Overall Anomaly MAE': avg_overall_anom_mae,\n",
    "        'Average Overall Anomaly RMSE': avg_overall_anom_rmse,\n",
    "        'Total Training Time': f\"{int(minutes)}m {int(seconds)}s\"\n",
    "    }\n",
    "\n",
    "    return summary_results, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dumb(test_df, scalers, lookback=LOOKBACK, horizon=HORIZON):\n",
    "    print(\"Preparing Baseline Test Data...\")\n",
    "\n",
    "    # Normalize data\n",
    "    test_df['minRSSI'] = scalers['scaler_target'].transform(test_df[['minRSSI']])\n",
    "\n",
    "    # Prepare test sequences\n",
    "    X_test, y_test, test_anomalies, test_cells = create_sequences(test_df, lookback, horizon)\n",
    "\n",
    "    # Baseline predictions: Repeat the last value from each sequence in the LOOKBACK period\n",
    "    y_pred_baseline = X_test[:, -1, 0].reshape(-1, 1).repeat(horizon, axis=1)\n",
    "\n",
    "    # Inverse transform predictions and actual values\n",
    "    y_test_original = scalers['scaler_target'].inverse_transform(y_test).reshape(-1, horizon)\n",
    "    y_pred_original = scalers['scaler_target'].inverse_transform(y_pred_baseline).reshape(-1, horizon)\n",
    "\n",
    "    # Calculate per-step MAE and RMSE\n",
    "    per_step_mae, per_step_rmse = [], []\n",
    "    per_step_anom_mae, per_step_anom_rmse = [], []\n",
    "\n",
    "    print(\"\\nEvaluating Baseline...\")\n",
    "    for step in range(horizon):\n",
    "        # Calculate general per-step metrics (MAE, RMSE)\n",
    "        mae_step = mean_absolute_error(y_test_original[:, step], y_pred_original[:, step])\n",
    "        rmse_step = np.sqrt(mean_squared_error(y_test_original[:, step], y_pred_original[:, step]))\n",
    "        per_step_mae.append(mae_step)\n",
    "        per_step_rmse.append(rmse_step)\n",
    "\n",
    "        # Anomaly-specific metrics (only considering values where anomaly == 1)\n",
    "        step_anomaly_mask = test_anomalies[:, step] == 1\n",
    "        if np.any(step_anomaly_mask):\n",
    "            anom_mae_step = mean_absolute_error(y_test_original[step_anomaly_mask, step], y_pred_original[step_anomaly_mask, step])\n",
    "            anom_rmse_step = np.sqrt(mean_squared_error(y_test_original[step_anomaly_mask, step], y_pred_original[step_anomaly_mask, step]))\n",
    "        else:\n",
    "            anom_mae_step, anom_rmse_step = np.nan, np.nan\n",
    "\n",
    "        per_step_anom_mae.append(anom_mae_step)\n",
    "        per_step_anom_rmse.append(anom_rmse_step)\n",
    "\n",
    "    # Calculate overall MAE and RMSE across all steps\n",
    "    overall_mae = np.mean(per_step_mae)\n",
    "    overall_rmse = np.mean(per_step_rmse)\n",
    "    overall_anom_mae = np.nanmean(per_step_anom_mae)\n",
    "    overall_anom_rmse = np.nanmean(per_step_anom_rmse)\n",
    "\n",
    "    print(f\"Baseline Test MAE: {overall_mae:.4f}, Test RMSE: {overall_rmse:.4f}\")\n",
    "    print(f\"Baseline Test Anomaly MAE: {overall_anom_mae:.4f}, Test Anomaly RMSE: {overall_anom_rmse:.4f}\")\n",
    "\n",
    "    # Create a DataFrame for predictions to save for plotting\n",
    "    predictions = []\n",
    "    for i in range(y_test_original.shape[0]):  # Iterate over each sample (cell)\n",
    "        row = {'cell_id': test_cells[i][0]}\n",
    "\n",
    "        for step in range(horizon):\n",
    "            row[f'actual_{step+1}'] = y_test_original[i, step]\n",
    "            row[f'predicted_{step+1}'] = y_pred_original[i, step]\n",
    "            row[f'anomaly_{step+1}'] = test_anomalies[i, step]\n",
    "\n",
    "        predictions.append(row)\n",
    "\n",
    "    # Create a DataFrame for predictions to save or plot later\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "    # Return the results dictionary (evaluation metrics) and predictions DataFrame\n",
    "    results = {\n",
    "        'MAE_per_step': per_step_mae,\n",
    "        'RMSE_per_step': per_step_rmse,\n",
    "        'Anom_MAE_per_step': per_step_anom_mae,\n",
    "        'Anom_RMSE_per_step': per_step_anom_rmse,\n",
    "        'Overall_MAE': overall_mae,\n",
    "        'Overall_RMSE': overall_rmse,\n",
    "        'Overall_Anomaly_MAE': overall_anom_mae,\n",
    "        'Overall_Anomaly_RMSE': overall_anom_rmse\n",
    "    }\n",
    "\n",
    "    return results, predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(pred_df_path, cell_id, horizon):\n",
    "    # Load the predictions DataFrame\n",
    "    pred_df = pd.read_csv(pred_df_path)\n",
    "    \n",
    "    # Filter the DataFrame for the specified cell_id\n",
    "    cell_data = pred_df[pred_df['cell_id'] == cell_id]\n",
    "    \n",
    "    # Extract the actual, predicted values and anomalies for the specified horizon\n",
    "    actual_col = f'actual_{horizon}'\n",
    "    predicted_col = f'predicted_{horizon}'\n",
    "    anomaly_col = f'anomaly_{horizon}'\n",
    "    \n",
    "    actual_values = cell_data[actual_col].values\n",
    "    predicted_values = cell_data[predicted_col].values\n",
    "    anomalies = cell_data[anomaly_col].values\n",
    "    \n",
    "    # Plot the actual and predicted values\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(actual_values, label='Actual', color='blue')\n",
    "    plt.plot(predicted_values, label='Predicted', color='orange')\n",
    "    \n",
    "    # Highlight anomalies with red dots\n",
    "    anomaly_indices = anomalies == 1\n",
    "    plt.scatter(np.arange(len(actual_values))[anomaly_indices], \n",
    "                actual_values[anomaly_indices], color='red', label='Anomaly', marker='o', s=30, edgecolors='k', zorder=5)\n",
    "\n",
    "    if horizon == 1:\n",
    "        time_step_desc = \"30 minutes\"\n",
    "    else:\n",
    "        time_step_desc = f\"{horizon * 0.5} hours\"  # Each horizon step is 30 minutes\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('minRSSI')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isvcolF8_kWu"
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LunGFPbs_ZEE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1:\n",
      "  Train set shape: (2059024, 11)\n",
      "  Validation set shape: (2059021, 11)\n",
      "Split 2:\n",
      "  Train set shape: (4118045, 11)\n",
      "  Validation set shape: (2059021, 11)\n",
      "Split 3:\n",
      "  Train set shape: (6177066, 11)\n",
      "  Validation set shape: (2059021, 11)\n",
      "Split 4:\n",
      "  Train set shape: (8236087, 11)\n",
      "  Validation set shape: (2059021, 11)\n",
      "Test set shape: (2573778, 11)\n"
     ]
    }
   ],
   "source": [
    "splits, test_set = time_series_split(df, 4)\n",
    "\n",
    "for i, (train, val) in enumerate(splits):\n",
    "    print(f\"Split {i + 1}:\")\n",
    "    print(f\"  Train set shape: {train.shape}\")\n",
    "    print(f\"  Validation set shape: {val.shape}\")\n",
    "\n",
    "print(f\"Test set shape: {test_set.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SueCamVvA7Wp"
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6qXfY5FjA8aH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Split 1/4\n",
      "X_train shape: (2042824, 1, 1), y_train shape: (2042824, 24)\n",
      "X_val shape: (2042821, 1, 1), y_val shape: (2042821, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hv/7c6y54956n9_w4470fs3q8c00000gn/T/ipykernel_5180/3847656879.py:49: RuntimeWarning: Mean of empty slice\n",
      "  'Anom_MAE': np.nanmean(anom_mae), 'Anom_RMSE': np.nanmean(anom_rmse)})\n",
      "/var/folders/hv/7c6y54956n9_w4470fs3q8c00000gn/T/ipykernel_5180/3847656879.py:52: RuntimeWarning: Mean of empty slice\n",
      "  f\"Anomaly MAE: {np.nanmean(anom_mae):.4f}, Anomaly RMSE: {np.nanmean(anom_rmse):.4f}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 - Overall MAE: 0.5396, Overall RMSE: 1.0366, Anomaly MAE: nan, Anomaly RMSE: nan\n",
      "\n",
      "Processing Split 2/4\n",
      "X_train shape: (4101845, 1, 1), y_train shape: (4101845, 24)\n",
      "X_val shape: (2042821, 1, 1), y_val shape: (2042821, 24)\n",
      "Split 2 - Overall MAE: 0.6287, Overall RMSE: 1.3189, Anomaly MAE: 17.5851, Anomaly RMSE: 26.6654\n",
      "\n",
      "Processing Split 3/4\n",
      "X_train shape: (6160866, 1, 1), y_train shape: (6160866, 24)\n",
      "X_val shape: (2042821, 1, 1), y_val shape: (2042821, 24)\n",
      "Split 3 - Overall MAE: 0.7537, Overall RMSE: 1.5877, Anomaly MAE: 13.4217, Anomaly RMSE: 14.4786\n",
      "\n",
      "Processing Split 4/4\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model across all splits\n",
    "summary_results, scalers = train_validate_dumb(splits, lookback=LOOKBACK, horizon=HORIZON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Split 1/1\n",
      "X_train shape: (8219887, 1, 1), y_train shape: (8219887, 24)\n",
      "X_val shape: (2042821, 1, 1), y_val shape: (2042821, 24)\n",
      "Split 1 - Overall MAE: 0.8064, Overall RMSE: 1.7110, Anomaly MAE: 12.3295, Anomaly RMSE: 13.8461\n",
      "\n",
      "Results for the Fourth Split:\n",
      "Average Overall MAE: 0.8064\n",
      "Average Overall RMSE: 1.7110\n",
      "Average Overall Anomaly MAE: 12.3295\n",
      "Average Overall Anomaly RMSE: 13.8461\n",
      "Total Training Time: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Run the train_validate_dumb function on the fourth split only\n",
    "split4 = [splits[3]]  # Extract the fourth split\n",
    "summary_results_split4, scalers = train_validate_dumb(split4, lookback=LOOKBACK, horizon=HORIZON)\n",
    "\n",
    "# Print the results for the fourth split\n",
    "print(\"\\nResults for the Fourth Split:\")\n",
    "for metric, value in summary_results_split4.items():\n",
    "    try:\n",
    "        print(f\"{metric}: {float(value):.4f}\")\n",
    "    except ValueError:\n",
    "        print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Baseline Test Data...\n",
      "\n",
      "Evaluating Baseline...\n",
      "Baseline Test MAE: 1.0169, Test RMSE: 2.1695\n",
      "Baseline Test Anomaly MAE: 9.4446, Test Anomaly RMSE: 10.8702\n"
     ]
    }
   ],
   "source": [
    "test_results, test_preds = evaluate_dumb(test_set, scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "uni_tilburg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
