{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dazA9FqwLJie"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras import utils\n",
    "\n",
    "utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "VjTCMXai53r8"
   },
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "LOOKBACK = 24\n",
    "HORIZON = 24\n",
    "N_SPLITS = 4\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "model_name='base_multi1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(933661, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>cell</th>\n",
       "      <th>bts</th>\n",
       "      <th>antenna</th>\n",
       "      <th>carrier</th>\n",
       "      <th>minRSSI</th>\n",
       "      <th>pageSessions</th>\n",
       "      <th>ULvol</th>\n",
       "      <th>sessionDur</th>\n",
       "      <th>blocks</th>\n",
       "      <th>...</th>\n",
       "      <th>Azimuth</th>\n",
       "      <th>SectorsPerBts</th>\n",
       "      <th>NearbyBts</th>\n",
       "      <th>Dist2Coast</th>\n",
       "      <th>ClusterId</th>\n",
       "      <th>CellsPerBts</th>\n",
       "      <th>OverallPageSessions</th>\n",
       "      <th>OverallULvol</th>\n",
       "      <th>OverallSessionDur</th>\n",
       "      <th>OverallBlocks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-02 00:30:00+00:00</td>\n",
       "      <td>580_0_0</td>\n",
       "      <td>580</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-105.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.279611</td>\n",
       "      <td>23.411765</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>39.189340</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>243284</td>\n",
       "      <td>3278.715746</td>\n",
       "      <td>151645.970902</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-02 00:30:00+00:00</td>\n",
       "      <td>580_2_0</td>\n",
       "      <td>580</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-106.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.306054</td>\n",
       "      <td>22.725490</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>250.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>39.189340</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>398291</td>\n",
       "      <td>5039.146780</td>\n",
       "      <td>185577.046230</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-02 00:30:00+00:00</td>\n",
       "      <td>580_2_1</td>\n",
       "      <td>580</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-105.75</td>\n",
       "      <td>52</td>\n",
       "      <td>0.110888</td>\n",
       "      <td>26.846154</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>250.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>39.189340</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>149548</td>\n",
       "      <td>2538.938010</td>\n",
       "      <td>216524.383783</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-09-02 00:30:00+00:00</td>\n",
       "      <td>579_2_1</td>\n",
       "      <td>579</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-106.95</td>\n",
       "      <td>35</td>\n",
       "      <td>0.105287</td>\n",
       "      <td>8.975000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>260.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>34.621906</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>224144</td>\n",
       "      <td>3234.243132</td>\n",
       "      <td>175408.135678</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-09-02 00:30:00+00:00</td>\n",
       "      <td>564_2_1</td>\n",
       "      <td>564</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-107.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080681</td>\n",
       "      <td>20.050000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>250.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.379122</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>154463</td>\n",
       "      <td>1777.783403</td>\n",
       "      <td>304683.070142</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp     cell  bts  antenna  carrier  minRSSI  \\\n",
       "0  2023-09-02 00:30:00+00:00  580_0_0  580        0        0  -105.18   \n",
       "1  2023-09-02 00:30:00+00:00  580_2_0  580        2        0  -106.30   \n",
       "2  2023-09-02 00:30:00+00:00  580_2_1  580        2        1  -105.75   \n",
       "3  2023-09-02 00:30:00+00:00  579_2_1  579        2        1  -106.95   \n",
       "4  2023-09-02 00:30:00+00:00  564_2_1  564        2        1  -107.88   \n",
       "\n",
       "   pageSessions     ULvol  sessionDur  blocks  ...  Azimuth  SectorsPerBts  \\\n",
       "0             0  0.279611   23.411765       0  ...     20.0            3.0   \n",
       "1             0  0.306054   22.725490       0  ...    250.0            3.0   \n",
       "2            52  0.110888   26.846154       0  ...    250.0            3.0   \n",
       "3            35  0.105287    8.975000       0  ...    260.0            3.0   \n",
       "4             0  0.080681   20.050000       0  ...    250.0            3.0   \n",
       "\n",
       "   NearbyBts  Dist2Coast  ClusterId  CellsPerBts  OverallPageSessions  \\\n",
       "0        9.0   39.189340          0            3               243284   \n",
       "1        9.0   39.189340          0            3               398291   \n",
       "2        9.0   39.189340          0            3               149548   \n",
       "3        8.0   34.621906          0            1               224144   \n",
       "4        1.0   47.379122          0            2               154463   \n",
       "\n",
       "   OverallULvol  OverallSessionDur  OverallBlocks  \n",
       "0   3278.715746      151645.970902              2  \n",
       "1   5039.146780      185577.046230              7  \n",
       "2   2538.938010      216524.383783              2  \n",
       "3   3234.243132      175408.135678              6  \n",
       "4   1777.783403      304683.070142              0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../../../config.json', 'r') as config_file:\n",
    "   config = json.load(config_file)\n",
    "\n",
    "df = pd.read_csv(os.path.join(config['data_path'], 'cell_multivar.csv'))\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_X = ['pageSessions', 'ULvol', 'sessionDur']\n",
    "static_X = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPkJYqv65-8O"
   },
   "source": [
    "# Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ldcRlogr5843"
   },
   "outputs": [],
   "source": [
    "# Time series split function (Expanding Window)\n",
    "def time_series_split(df, n_splits=N_SPLITS, test_size=0.2):\n",
    "    df = df.sort_values('timestamp')\n",
    "    test_split_index = int(len(df) * (1 - test_size))\n",
    "    train_val_df = df.iloc[:test_split_index]\n",
    "    test_df = df.iloc[test_split_index:]\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    splits = [(train_val_df.iloc[train_index], train_val_df.iloc[val_index]) for train_index, val_index in tscv.split(train_val_df)]\n",
    "    return splits, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6oS3gb2T6Po6"
   },
   "outputs": [],
   "source": [
    "# Sequence creation for univariate time series\n",
    "def create_sequences(df, lookback=LOOKBACK, horizon=HORIZON):\n",
    "    X, y, anomaly, cell_id = [], [], [], []\n",
    "\n",
    "    # Loop through each unique cell in the dataset\n",
    "    for cell in df['cell'].unique():\n",
    "        # Filter the dataframe for the current cell only\n",
    "        cell_df = df[df['cell'] == cell]\n",
    "\n",
    "        # Generate sequences within this cell's data\n",
    "        for i in range(lookback, len(cell_df) - horizon + 1):\n",
    "            # Lookback sequence for minRSSI only (univariate)\n",
    "            X_seq = cell_df.iloc[i - lookback:i][['minRSSI']].values\n",
    "            # Target horizon sequence for minRSSI\n",
    "            y_seq = cell_df.iloc[i:i + horizon]['minRSSI'].values\n",
    "            # Anomaly sequences for later evaluation\n",
    "            anomaly_seq = cell_df.iloc[i:i + horizon]['anomaly'].values\n",
    "            # Cell ID for each sequence\n",
    "            cell_seq = cell_df.iloc[i:i + horizon]['cell'].values\n",
    "\n",
    "            # Append sequences to output lists\n",
    "            X.append(X_seq)\n",
    "            y.append(y_seq)\n",
    "            anomaly.append(anomaly_seq)\n",
    "            cell_id.append(cell_seq)\n",
    "\n",
    "    # Convert lists to numpy arrays for model input\n",
    "    return np.array(X), np.array(y), np.array(anomaly), np.array(cell_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data_split(train_df, val_df, temporal_features=temporal_X, static_features=static_X):\n",
    "    scaler_temporal = StandardScaler()\n",
    "    scaler_static = MinMaxScaler()\n",
    "    scaler_target = StandardScaler()\n",
    "\n",
    "    # Scale time-variant features\n",
    "    if temporal_features:\n",
    "        train_df[temporal_features] = scaler_temporal.fit_transform(train_df[temporal_features])\n",
    "        val_df[temporal_features] = scaler_temporal.transform(val_df[temporal_features])\n",
    "\n",
    "    # Scale time-invariant features\n",
    "    if static_features:\n",
    "        train_df[static_features] = scaler_static.fit_transform(train_df[static_features])\n",
    "        val_df[static_features] = scaler_static.transform(val_df[static_features])\n",
    "\n",
    "    # Scale minRSSI separately (target variable)\n",
    "    train_df['minRSSI'] = scaler_target.fit_transform(train_df[['minRSSI']])\n",
    "    val_df['minRSSI'] = scaler_target.transform(val_df[['minRSSI']])\n",
    "\n",
    "    return train_df, val_df, scaler_target, scaler_temporal, scaler_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "MbE4f9PV8-fP"
   },
   "outputs": [],
   "source": [
    "def train_validate_dumb(splits, lookback=LOOKBACK, horizon=HORIZON):\n",
    "    results = []\n",
    "    scalers = {}\n",
    "    total_training_time = 0\n",
    "\n",
    "    for i, (train_df, val_df) in enumerate(splits):\n",
    "        print(f\"\\nProcessing Split {i + 1}/{len(splits)}\")\n",
    "\n",
    "        # Scale the current split\n",
    "        scaled_train, scaled_val, scaler_target, scaler_temporal, scaler_static = scale_data_split(train_df.copy(), val_df.copy())\n",
    "        scalers = {'scaler_target': scaler_target, 'scaler_temporal': scaler_temporal, 'scaler_static': scaler_static}\n",
    "\n",
    "        # Create sequences\n",
    "        X_train, y_train, _, _ = create_sequences(scaled_train, LOOKBACK, HORIZON)\n",
    "        X_val, y_val, val_anomalies, _ = create_sequences(scaled_val, LOOKBACK, HORIZON)\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "\n",
    "        # Start timer\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Dummy model predictions for validation (use the last minRSSI value)\n",
    "        y_pred_baseline = X_val[:, -1, 0].reshape(-1, 1).repeat(HORIZON, axis=1)\n",
    "\n",
    "        # End timer\n",
    "        split_training_time = time.time() - start_time\n",
    "        total_training_time += split_training_time\n",
    "\n",
    "        # Evaluate results\n",
    "        y_val_og = scalers['scaler_target'].inverse_transform(y_val)\n",
    "        y_pred_og = scalers['scaler_target'].inverse_transform(y_pred_baseline)\n",
    "\n",
    "        mae = mean_absolute_error(y_val_og, y_pred_og)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_og, y_pred_og))\n",
    "\n",
    "        # Anomaly-based evaluation\n",
    "        anom_mae, anom_rmse = [], []\n",
    "        for step in range(HORIZON):\n",
    "            step_anomaly_mask = val_anomalies[:, step] == 1\n",
    "            if np.any(step_anomaly_mask):\n",
    "                anom_mae.append(mean_absolute_error(y_val_og[step_anomaly_mask, step], y_pred_og[step_anomaly_mask, step]))\n",
    "                anom_rmse.append(np.sqrt(mean_squared_error(y_val_og[step_anomaly_mask, step], y_pred_og[step_anomaly_mask, step])))\n",
    "            else:\n",
    "                anom_mae.append(np.nan)\n",
    "                anom_rmse.append(np.nan)\n",
    "\n",
    "        results.append({'split': i + 1, 'Overall_MAE': mae, 'Overall_RMSE': rmse,\n",
    "                        'Anom_MAE': np.nanmean(anom_mae), 'Anom_RMSE': np.nanmean(anom_rmse)})\n",
    "\n",
    "        print(f\"Split {i + 1} - Overall MAE: {mae:.4f}, Overall RMSE: {rmse:.4f}, \"\n",
    "              f\"Anomaly MAE: {np.nanmean(anom_mae):.4f}, Anomaly RMSE: {np.nanmean(anom_rmse):.4f}\")\n",
    "\n",
    "    # Convert total training time to minutes and seconds format\n",
    "    minutes, seconds = divmod(total_training_time, 60)\n",
    "\n",
    "    # Aggregate results\n",
    "    avg_overall_mae = np.mean([res['Overall_MAE'] for res in results])\n",
    "    avg_overall_rmse = np.mean([res['Overall_RMSE'] for res in results])\n",
    "    avg_overall_anom_mae = np.nanmean([res['Anom_MAE'] for res in results])\n",
    "    avg_overall_anom_rmse = np.nanmean([res['Anom_RMSE'] for res in results])\n",
    "\n",
    "    summary_results = {\n",
    "        'Average Overall MAE': avg_overall_mae,\n",
    "        'Average Overall RMSE': avg_overall_rmse,\n",
    "        'Average Overall Anomaly MAE': avg_overall_anom_mae,\n",
    "        'Average Overall Anomaly RMSE': avg_overall_anom_rmse,\n",
    "        'Total Training Time': f\"{int(minutes)}m {int(seconds)}s\"\n",
    "    }\n",
    "\n",
    "    return summary_results, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dumb(test_df, scalers=scalers, lookback=LOOKBACK, horizon=HORIZON):\n",
    "    print(\"Preparing Baseline Test Data...\")\n",
    "\n",
    "    # Normalize data\n",
    "    test_df['minRSSI'] = scalers['scaler_target'].transform(test_df[['minRSSI']])\n",
    "\n",
    "    # Prepare test sequences\n",
    "    X_test, y_test, test_anomalies, test_cells = create_sequences(test_df, lookback, horizon)\n",
    "\n",
    "    # Baseline predictions: Repeat the last value from each sequence in the LOOKBACK period\n",
    "    y_pred_baseline = X_test[:, -1, 0].reshape(-1, 1).repeat(horizon, axis=1)\n",
    "\n",
    "    # Inverse transform predictions and actual values\n",
    "    y_test_original = scalers['scaler_target'].inverse_transform(y_test).reshape(-1, horizon)\n",
    "    y_pred_original = scalers['scaler_target'].inverse_transform(y_pred_baseline).reshape(-1, horizon)\n",
    "\n",
    "    # Calculate per-step MAE and RMSE\n",
    "    per_step_mae, per_step_rmse = [], []\n",
    "    per_step_anom_mae, per_step_anom_rmse = [], []\n",
    "\n",
    "    print(\"\\nEvaluating Baseline...\")\n",
    "    for step in range(horizon):\n",
    "        # Calculate general per-step metrics (MAE, RMSE)\n",
    "        mae_step = mean_absolute_error(y_test_original[:, step], y_pred_original[:, step])\n",
    "        rmse_step = np.sqrt(mean_squared_error(y_test_original[:, step], y_pred_original[:, step]))\n",
    "        per_step_mae.append(mae_step)\n",
    "        per_step_rmse.append(rmse_step)\n",
    "\n",
    "        # Anomaly-specific metrics (only considering values where anomaly == 1)\n",
    "        step_anomaly_mask = test_anomalies[:, step] == 1\n",
    "        if np.any(step_anomaly_mask):\n",
    "            anom_mae_step = mean_absolute_error(y_test_original[step_anomaly_mask, step], y_pred_original[step_anomaly_mask, step])\n",
    "            anom_rmse_step = np.sqrt(mean_squared_error(y_test_original[step_anomaly_mask, step], y_pred_original[step_anomaly_mask, step]))\n",
    "        else:\n",
    "            anom_mae_step, anom_rmse_step = np.nan, np.nan\n",
    "\n",
    "        per_step_anom_mae.append(anom_mae_step)\n",
    "        per_step_anom_rmse.append(anom_rmse_step)\n",
    "\n",
    "    # Calculate overall MAE and RMSE across all steps\n",
    "    overall_mae = np.mean(per_step_mae)\n",
    "    overall_rmse = np.mean(per_step_rmse)\n",
    "    overall_anom_mae = np.nanmean(per_step_anom_mae)\n",
    "    overall_anom_rmse = np.nanmean(per_step_anom_rmse)\n",
    "\n",
    "    print(f\"Baseline Test MAE: {overall_mae:.4f}, Test RMSE: {overall_rmse:.4f}\")\n",
    "    print(f\"Baseline Test Anomaly MAE: {overall_anom_mae:.4f}, Test Anomaly RMSE: {overall_anom_rmse:.4f}\")\n",
    "\n",
    "    # Create a DataFrame for predictions to save for plotting\n",
    "    predictions = []\n",
    "    for i in range(y_test_original.shape[0]):  # Iterate over each sample (cell)\n",
    "        row = {'cell_id': test_cells[i][0]}\n",
    "\n",
    "        for step in range(horizon):\n",
    "            row[f'actual_{step+1}'] = y_test_original[i, step]\n",
    "            row[f'predicted_{step+1}'] = y_pred_original[i, step]\n",
    "            row[f'anomaly_{step+1}'] = test_anomalies[i, step]\n",
    "\n",
    "        predictions.append(row)\n",
    "\n",
    "    # Create a DataFrame for predictions to save or plot later\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "    # Return the results dictionary (evaluation metrics) and predictions DataFrame\n",
    "    results = {\n",
    "        'MAE_per_step': per_step_mae,\n",
    "        'RMSE_per_step': per_step_rmse,\n",
    "        'Anom_MAE_per_step': per_step_anom_mae,\n",
    "        'Anom_RMSE_per_step': per_step_anom_rmse,\n",
    "        'Overall_MAE': overall_mae,\n",
    "        'Overall_RMSE': overall_rmse,\n",
    "        'Overall_Anomaly_MAE': overall_anom_mae,\n",
    "        'Overall_Anomaly_RMSE': overall_anom_rmse\n",
    "    }\n",
    "\n",
    "    return results, predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(pred_df_path, cell_id, horizon):\n",
    "    # Load the predictions DataFrame\n",
    "    pred_df = pd.read_csv(pred_df_path)\n",
    "    \n",
    "    # Filter the DataFrame for the specified cell_id\n",
    "    cell_data = pred_df[pred_df['cell_id'] == cell_id]\n",
    "    \n",
    "    # Extract the actual, predicted values and anomalies for the specified horizon\n",
    "    actual_col = f'actual_{horizon}'\n",
    "    predicted_col = f'predicted_{horizon}'\n",
    "    anomaly_col = f'anomaly_{horizon}'\n",
    "    \n",
    "    actual_values = cell_data[actual_col].values\n",
    "    predicted_values = cell_data[predicted_col].values\n",
    "    anomalies = cell_data[anomaly_col].values\n",
    "    \n",
    "    # Plot the actual and predicted values\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(actual_values, label='Actual', color='blue')\n",
    "    plt.plot(predicted_values, label='Predicted', color='orange')\n",
    "    \n",
    "    # Highlight anomalies with red dots\n",
    "    anomaly_indices = anomalies == 1\n",
    "    plt.scatter(np.arange(len(actual_values))[anomaly_indices], \n",
    "                actual_values[anomaly_indices], color='red', label='Anomaly', marker='o', s=30, edgecolors='k', zorder=5)\n",
    "\n",
    "    if horizon == 1:\n",
    "        time_step_desc = \"30 minutes\"\n",
    "    else:\n",
    "        time_step_desc = f\"{horizon * 0.5} hours\"  # Each horizon step is 30 minutes\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('minRSSI')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isvcolF8_kWu"
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "LunGFPbs_ZEE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1:\n",
      "  Train set shape: (149388, 24)\n",
      "  Validation set shape: (149385, 24)\n",
      "Split 2:\n",
      "  Train set shape: (298773, 24)\n",
      "  Validation set shape: (149385, 24)\n",
      "Split 3:\n",
      "  Train set shape: (448158, 24)\n",
      "  Validation set shape: (149385, 24)\n",
      "Split 4:\n",
      "  Train set shape: (597543, 24)\n",
      "  Validation set shape: (149385, 24)\n",
      "Test set shape: (186733, 24)\n"
     ]
    }
   ],
   "source": [
    "splits, test_set = time_series_split(df, 4)\n",
    "\n",
    "for i, (train, val) in enumerate(splits):\n",
    "    print(f\"Split {i + 1}:\")\n",
    "    print(f\"  Train set shape: {train.shape}\")\n",
    "    print(f\"  Validation set shape: {val.shape}\")\n",
    "\n",
    "print(f\"Test set shape: {test_set.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SueCamVvA7Wp"
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6qXfY5FjA8aH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Split 1/4\n",
      "X_train shape: (136369, 24, 1), y_train shape: (136369, 24)\n",
      "X_val shape: (136366, 24, 1), y_val shape: (136366, 24)\n",
      "Split 1 - Overall MAE: 1.8744, Overall RMSE: 3.7729, Anomaly MAE: 12.5310, Anomaly RMSE: 13.6063\n",
      "\n",
      "Processing Split 2/4\n",
      "X_train shape: (285754, 24, 1), y_train shape: (285754, 24)\n",
      "X_val shape: (136366, 24, 1), y_val shape: (136366, 24)\n",
      "Split 2 - Overall MAE: 0.8096, Overall RMSE: 1.8460, Anomaly MAE: 14.3443, Anomaly RMSE: 19.0582\n",
      "\n",
      "Processing Split 3/4\n",
      "X_train shape: (435139, 24, 1), y_train shape: (435139, 24)\n",
      "X_val shape: (136366, 24, 1), y_val shape: (136366, 24)\n",
      "Split 3 - Overall MAE: 1.4435, Overall RMSE: 2.9181, Anomaly MAE: 13.5432, Anomaly RMSE: 14.5206\n",
      "\n",
      "Processing Split 4/4\n",
      "X_train shape: (584524, 24, 1), y_train shape: (584524, 24)\n",
      "X_val shape: (136366, 24, 1), y_val shape: (136366, 24)\n",
      "Split 4 - Overall MAE: 2.4260, Overall RMSE: 4.2522, Anomaly MAE: 8.7018, Anomaly RMSE: 10.3989\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model across all splits\n",
    "summary_results, scalers = train_validate_dumb(splits, lookback=LOOKBACK, horizon=HORIZON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Results:\n",
      "Average Overall MAE: 1.6384\n",
      "Average Overall RMSE: 3.1973\n",
      "Average Overall Anomaly MAE: 12.2801\n",
      "Average Overall Anomaly RMSE: 14.3960\n",
      "Total Training Time: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCross-Validation Results:\")\n",
    "for metric, value in summary_results.items():\n",
    "    try:\n",
    "        print(f\"{metric}: {float(value):.4f}\")\n",
    "    except ValueError:\n",
    "        print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same results as the univariate model so no additional evaluation required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "uni_tilburg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
