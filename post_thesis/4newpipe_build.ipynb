{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1732783515210,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "dazA9FqwLJie"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv1D, Dense, Dropout, Input, MaxPooling1D, Flatten\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732783126307,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "VjTCMXai53r8"
   },
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "LOOKBACK = 4\n",
    "HORIZON = 4\n",
    "N_SPLITS = 2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "anomaly_weight = 0.001\n",
    "model_name = 'cnn_univar1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isvcolF8_kWu"
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(395914, 12)\n",
      "Index(['timestamp', 'cell', 'bts', 'antenna', 'carrier', 'minRSSI',\n",
      "       'pageSessions', 'ULvol', 'sessionSetupDur', 'sessionDur', 'blocks',\n",
      "       'anomaly'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "with open('../config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "df = pd.read_csv(os.path.join(config['data_path'], 'cell_undersampled_2.csv'))\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8197, 12)\n",
      "\n",
      "cell\n",
      "238_0_1    0.121\n",
      "341_2_0    0.129\n",
      "599_1_0    0.111\n",
      "Name: anomaly, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Sample 3 unique cells from the dataframe\n",
    "sampled_cells = df['cell'].drop_duplicates().sample(n=3, random_state=42)\n",
    "mini_df = df[df['cell'].isin(sampled_cells)]\n",
    "\n",
    "print(mini_df.shape)\n",
    "print()\n",
    "print(mini_df.groupby('cell')['anomaly'].mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1732783131367,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "OYeeCcNIUd_F"
   },
   "outputs": [],
   "source": [
    "temporal_X = []\n",
    "static_X = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPkJYqv65-8O"
   },
   "source": [
    "# Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1732783131367,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "ldcRlogr5843"
   },
   "outputs": [],
   "source": [
    "# Time series split function (Expanding Window)\n",
    "def time_series_split(df, n_splits=N_SPLITS, test_size=0.2):\n",
    "    df = df.sort_values('timestamp')\n",
    "    test_split_index = int(len(df) * (1 - test_size))\n",
    "    train_val_df = df.iloc[:test_split_index]\n",
    "    test_df = df.iloc[test_split_index:]\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    splits = [(train_val_df.iloc[train_index], train_val_df.iloc[val_index]) for train_index, val_index in tscv.split(train_val_df)]\n",
    "    return splits, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, lookback=LOOKBACK, horizon=HORIZON, static_X=static_X, temporal_X=temporal_X):\n",
    "    X, y, cell_id = [], [], []\n",
    "\n",
    "    for cell in df['cell'].unique():\n",
    "        cell_df = df[df['cell'] == cell]\n",
    "\n",
    "        for i in range(lookback, len(cell_df) - horizon + 1):\n",
    "            # Time-variant features\n",
    "            X_seq = cell_df.iloc[i - lookback:i][['minRSSI'] + temporal_X].values\n",
    "\n",
    "            # Static features (replicated across lookback steps)\n",
    "            static_seq = (\n",
    "                np.tile(cell_df.iloc[i][static_X].values, (lookback, 1))\n",
    "                if static_X else []\n",
    "            )\n",
    "\n",
    "            # Combine features\n",
    "            X_combined = np.concatenate([X_seq, static_seq], axis=1) if static_X else X_seq\n",
    "\n",
    "            # Target and anomaly labels\n",
    "            y_seq = cell_df.iloc[i:i + horizon]['minRSSI'].values\n",
    "            anomaly_seq = cell_df.iloc[i:i + horizon]['anomaly'].values\n",
    "\n",
    "            # Append anomaly indicator to the target\n",
    "            y_seq_with_anomaly = np.column_stack((y_seq, anomaly_seq))\n",
    "\n",
    "            X.append(X_combined)\n",
    "            y.append(y_seq_with_anomaly)\n",
    "            cell_id.append(cell_df.iloc[i:i + horizon]['cell'].values)\n",
    "\n",
    "    print(f\"Sequences created: X shape = {np.array(X).shape}, y shape = {np.array(y).shape}\", end='\\n\\n')\n",
    "    return np.array(X), np.array(y), np.array(cell_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1732783131367,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "FSmSO9zRSSu-"
   },
   "outputs": [],
   "source": [
    "def scale_data_split(train_df, val_df, temporal_features=temporal_X, static_features=static_X):\n",
    "    scaler_temporal = StandardScaler()\n",
    "    scaler_static = MinMaxScaler()\n",
    "    scaler_target = StandardScaler()\n",
    "\n",
    "    # Scale time-variant features\n",
    "    if temporal_features:\n",
    "        train_df[temporal_features] = scaler_temporal.fit_transform(train_df[temporal_features])\n",
    "        val_df[temporal_features] = scaler_temporal.transform(val_df[temporal_features])\n",
    "\n",
    "    # Scale time-invariant features\n",
    "    if static_features:\n",
    "        train_df[static_features] = scaler_static.fit_transform(train_df[static_features])\n",
    "        val_df[static_features] = scaler_static.transform(val_df[static_features])\n",
    "\n",
    "    # Scale minRSSI separately (target variable)\n",
    "    train_df['minRSSI'] = scaler_target.fit_transform(train_df[['minRSSI']])\n",
    "    val_df['minRSSI'] = scaler_target.transform(val_df[['minRSSI']])\n",
    "\n",
    "    return train_df, val_df, scaler_target, scaler_temporal, scaler_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mae(y_true, y_pred, weights):\n",
    "    # Extract the minRSSI values (first column in y_true)\n",
    "    y_true_values = y_true[:, :, 0]  # minRSSI values\n",
    "    anomaly_mask = y_true[:, :, 1]   # Anomaly indicators (0 or 1)\n",
    "\n",
    "    # Compute the absolute error\n",
    "    mae = tf.abs(y_true_values - y_pred)\n",
    "\n",
    "    # Apply the weighting for anomalies\n",
    "    weight_matrix = 1 + anomaly_mask * (weights - 1)  # Regular instances: weight=1, Anomalies: weight=weights\n",
    "    weighted_mae = mae * weight_matrix\n",
    "\n",
    "    # Return the mean weighted MAE\n",
    "    return tf.reduce_mean(weighted_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1732783537085,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "rHHpMuv8MBqO"
   },
   "outputs": [],
   "source": [
    "def build_cnn(lookback, horizon, n_features, anomaly_weight=1.0):\n",
    "    print(\"build_cnn activated\")\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(lookback, n_features)))\n",
    "\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(horizon))  # Outputs horizon predictions\n",
    "\n",
    "    # Compile with the custom weighted loss function\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=lambda y_true, y_pred: weighted_mae(y_true, y_pred, weights=anomaly_weight),\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1732783131367,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "IGmfE83aRyHC"
   },
   "outputs": [],
   "source": [
    "def train_validate(splits, lookback, horizon, anomaly_weight=1.0):\n",
    "    results = []\n",
    "    scalers = {}\n",
    "    total_training_time = 0\n",
    "    threshold = 0.1  # Example threshold for anomaly detection\n",
    "\n",
    "    for i, (train_df, val_df) in enumerate(splits):\n",
    "        print(f\"Processing Split {i + 1}/{len(splits)}\")\n",
    "\n",
    "        # Scale data\n",
    "        scaled_train, scaled_val, scaler_target, scaler_temporal, scaler_static = scale_data_split(\n",
    "            train_df.copy(), val_df.copy()\n",
    "        )\n",
    "        print(f\"Scaled train shape: {scaled_train.shape}, Scaled val shape: {scaled_val.shape}\")\n",
    "\n",
    "        scalers = {\n",
    "            'scaler_target': scaler_target,\n",
    "            'scaler_temporal': scaler_temporal,\n",
    "            'scaler_static': scaler_static\n",
    "        }\n",
    "\n",
    "        # Create sequences\n",
    "        X_train, y_train, _ = create_sequences(scaled_train, LOOKBACK, HORIZON)\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        X_val, y_val, _ = create_sequences(scaled_val, LOOKBACK, HORIZON)\n",
    "\n",
    "        # Ensure data type compatibility\n",
    "        X_train, y_train = X_train.astype(np.float32), y_train.astype(np.float32)\n",
    "        X_val, y_val = X_val.astype(np.float32), y_val.astype(np.float32)\n",
    "\n",
    "        n_features = X_train.shape[2]\n",
    "\n",
    "        # Build the model\n",
    "        model = build_cnn(lookback, horizon, n_features, anomaly_weight)\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        # Start timer\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # End timer\n",
    "        split_training_time = time.time() - start_time\n",
    "        total_training_time += split_training_time\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_val_actual = y_val[:, :, 0]  # Extract actual values from y_val (excluding anomaly indicator)\n",
    "        y_pred_actual = y_pred\n",
    "        y_val_anomaly = y_val[:, :, 1]  # Extract anomaly indicator\n",
    "\n",
    "        # Overall MAE and RMSE\n",
    "        mae = mean_absolute_error(y_val_actual, y_pred_actual)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_actual, y_pred_actual))\n",
    "\n",
    "        # Anomalous MAE and RMSE\n",
    "        anomalous_indices = y_val_anomaly == 1\n",
    "        mae_anom = mean_absolute_error(y_val_actual[anomalous_indices], y_pred_actual[anomalous_indices])\n",
    "        rmse_anom = np.sqrt(mean_squared_error(y_val_actual[anomalous_indices], y_pred_actual[anomalous_indices]))\n",
    "\n",
    "        # Binary anomaly predictions\n",
    "        anomaly_predicted = np.abs(y_val_actual - y_pred_actual) > threshold\n",
    "\n",
    "        # Confusion Matrix\n",
    "        tp = np.sum((anomaly_predicted & (y_val_anomaly == 1)))\n",
    "        fp = np.sum((anomaly_predicted & (y_val_anomaly == 0)))\n",
    "        tn = np.sum((~anomaly_predicted & (y_val_anomaly == 0)))\n",
    "        fn = np.sum((~anomaly_predicted & (y_val_anomaly == 1)))\n",
    "\n",
    "        # Recall\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "        results.append({\n",
    "            'split': i + 1,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAE_anom': mae_anom,\n",
    "            'RMSE_anom': rmse_anom,\n",
    "            'TP': tp,\n",
    "            'FP': fp,\n",
    "            'TN': tn,\n",
    "            'FN': fn,\n",
    "            'Recall': recall\n",
    "        })\n",
    "\n",
    "    # Summarize\n",
    "    avg_mae = np.mean([res['MAE'] for res in results])\n",
    "    avg_rmse = np.mean([res['RMSE'] for res in results])\n",
    "    avg_mae_anom = np.mean([res['MAE_anom'] for res in results])\n",
    "    avg_rmse_anom = np.mean([res['RMSE_anom'] for res in results])\n",
    "    avg_recall = np.mean([res['Recall'] for res in results])\n",
    "\n",
    "    summary_results = {\n",
    "        'Average MAE': avg_mae,\n",
    "        'Average RMSE': avg_rmse,\n",
    "        'Average MAE_anom': avg_mae_anom,\n",
    "        'Average RMSE_anom': avg_rmse_anom,\n",
    "        'Average Recall': avg_recall,\n",
    "        'Total Training Time': f\"{total_training_time // 60}m {total_training_time % 60:.2f}s\"\n",
    "    }\n",
    "\n",
    "    return summary_results, model, scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SueCamVvA7Wp"
   },
   "source": [
    "# Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1242,
     "status": "ok",
     "timestamp": 1732783170840,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "QOKsYCyqX5cT",
    "outputId": "4814cbe9-cd90-4f15-e7ab-7fb500997c23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1:\n",
      "  Train set shape: (2187, 12)\n",
      "  Validation set shape: (2185, 12)\n",
      "Split 2:\n",
      "  Train set shape: (4372, 12)\n",
      "  Validation set shape: (2185, 12)\n",
      "Test set shape: (1640, 12)\n"
     ]
    }
   ],
   "source": [
    "splits, test_set = time_series_split(mini_df, N_SPLITS)\n",
    "\n",
    "for i, (train, val) in enumerate(splits):\n",
    "    print(f\"Split {i + 1}:\")\n",
    "    print(f\"  Train set shape: {train.shape}\")\n",
    "    print(f\"  Validation set shape: {val.shape}\")\n",
    "\n",
    "print(f\"Test set shape: {test_set.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 537801,
     "status": "ok",
     "timestamp": 1732786162291,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "KOc8oBFLcwnm",
    "outputId": "85532b6d-8fee-4698-aeb5-a0700cbdcc12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Split 1/2\n",
      "Scaled train shape: (2187, 12), Scaled val shape: (2185, 12)\n",
      "Sequences created: X shape = (2166, 4, 1), y shape = (2166, 4, 2)\n",
      "\n",
      "X_train shape: (2166, 4, 1), y_train shape: (2166, 4, 2)\n",
      "Sequences created: X shape = (2164, 4, 1), y shape = (2164, 4, 2)\n",
      "\n",
      "build_cnn activated\n",
      "Epoch 1/2\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6577 - val_loss: 0.1449\n",
      "Epoch 2/2\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3236 - val_loss: 0.1356\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step\n",
      "Processing Split 2/2\n",
      "Scaled train shape: (4372, 12), Scaled val shape: (2185, 12)\n",
      "Sequences created: X shape = (4351, 4, 1), y shape = (4351, 4, 2)\n",
      "\n",
      "X_train shape: (4351, 4, 1), y_train shape: (4351, 4, 2)\n",
      "Sequences created: X shape = (2164, 4, 1), y shape = (2164, 4, 2)\n",
      "\n",
      "build_cnn activated\n",
      "Epoch 1/2\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4830 - val_loss: 0.2956\n",
      "Epoch 2/2\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - loss: 0.2491 - val_loss: 0.2818\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model across all splits\n",
    "summary_results, model, scalers = train_validate(splits, LOOKBACK, HORIZON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1732786162299,
     "user": {
      "displayName": "Jakob Tjurlik",
      "userId": "10339029674418762018"
     },
     "user_tz": -60
    },
    "id": "4FX6zPdKqzA3",
    "outputId": "b61ec822-cfae-4b88-ccdb-3eeb13035a03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Results:\n",
      "Average MAE: 0.2065\n",
      "Average RMSE: 0.3924\n",
      "Average MAE (Anomalies): 0.8375\n",
      "Average RMSE (Anomalies): 1.0247\n",
      "Average Recall: 0.9237\n",
      "Total Training Time: 0.0m 2.13s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCross-Validation Results:\")\n",
    "for metric, value in summary_results.items():\n",
    "    try:\n",
    "        print(f\"{metric}: {float(value):.4f}\")\n",
    "    except ValueError:\n",
    "        print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "uni_tilburg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
